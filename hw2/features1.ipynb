{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41234c0",
   "metadata": {},
   "source": [
    "# <center> Homework 2 - Dialog Systems and Acts (100 points) <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2f38d",
   "metadata": {},
   "source": [
    "<center> Jinzi Luo <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958c2ac",
   "metadata": {},
   "source": [
    "I confirm that I have not used any GPT-generated responses\n",
    "for any part of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3eebc4",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28405609",
   "metadata": {},
   "source": [
    "Extract two feature sets that you feel would be useful for the DAR problem. One feature\n",
    "set should be text-based, and the other feature set should be speech-based. Save\n",
    "text-based and speech-based feature sets as text_features_{train, valid, test}.csv and\n",
    "speech_features_{train, valid, test}.csv, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7a3495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (2.3.3)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: praat-parselmouth in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.62.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (from librosa) (5.2.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp312-abi3-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (from librosa) (4.15.0)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting cffi>=1.0 (from soundfile)\n",
      "  Downloading cffi-2.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Requirement already satisfied: packaging in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.45.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Collecting requests>=2.19.0 (from pooch>=1.1->librosa)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Downloading cffi-2.0.0-cp312-cp312-macosx_11_0_arm64.whl (181 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.2-cp312-cp312-macosx_11_0_arm64.whl (85 kB)\n",
      "Downloading numba-0.62.1-cp312-cp312-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.1-cp312-cp312-macosx_11_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl (288 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp312-abi3-macosx_11_0_arm64.whl (163 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: urllib3, tqdm, threadpoolctl, soxr, scipy, regex, pycparser, msgpack, llvmlite, lazy_loader, joblib, idna, click, charset_normalizer, certifi, audioread, scikit-learn, requests, numba, nltk, cffi, soundfile, pooch, librosa\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/24\u001b[0m [librosa]9/24\u001b[0m [nltk]]-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed audioread-3.1.0 certifi-2025.10.5 cffi-2.0.0 charset_normalizer-3.4.4 click-8.3.0 idna-3.11 joblib-1.5.2 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.45.1 msgpack-1.1.2 nltk-3.9.2 numba-0.62.1 pooch-1.8.2 pycparser-2.23 regex-2025.11.3 requests-2.32.5 scikit-learn-1.7.2 scipy-1.16.3 soundfile-0.13.1 soxr-1.0.0 threadpoolctl-3.6.0 tqdm-4.67.1 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn nltk librosa soundfile praat-parselmouth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bf7cd",
   "metadata": {},
   "source": [
    "### Text-based feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf35adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kenzieluo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kenzieluo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/kenzieluo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 下载 NLTK 必需资源\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "OUT_DIR = \"./task1\"\n",
    "CACHE_DIR = \"./cache_embed\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "ID_COLS = [\"dialog_id\", \"speaker\", \"da_tag\", \"start_time\", \"end_time\"]\n",
    "TEXT_COL = \"transcript\"   # 如果你数据叫 utterance/text，自行改掉这里\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# TF-IDF 参数\n",
    "NGRAM_RANGE = (1, 2)\n",
    "TFIDF_MAX_FEATURES = 2000  \n",
    "nltk.data.path.append(\"/Users/kenzieluo/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "460c79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def structural_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return {\n",
    "        \"len_chars\": len(text),\n",
    "        \"len_tokens\": len(tokens),\n",
    "        \"num_punct\": sum(1 for c in text if c in \".,?!;:\"),\n",
    "        \"ends_question\": int(text.strip().endswith(\"?\")),\n",
    "    }\n",
    "\n",
    "def pos_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    buckets = {\"NOUN\":0, \"VERB\":0, \"ADJ\":0, \"ADV\":0}\n",
    "    for _, tag in tagged:\n",
    "        if tag.startswith(\"NN\"): buckets[\"NOUN\"] += 1\n",
    "        elif tag.startswith(\"VB\"): buckets[\"VERB\"] += 1\n",
    "        elif tag.startswith(\"JJ\"): buckets[\"ADJ\"] += 1\n",
    "        elif tag.startswith(\"RB\"): buckets[\"ADV\"] += 1\n",
    "\n",
    "    total = len(tokens) if len(tokens) > 0 else 1\n",
    "    return {f\"pos_prop_{k}\": buckets[k]/total for k in buckets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b257fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "valid = pd.read_csv(os.path.join(DATA_DIR, \"valid.csv\"))\n",
    "test  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=TFIDF_MAX_FEATURES,\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    lowercase=True,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "\n",
    "# Fit TF-IDF on train\n",
    "tfidf_train = vectorizer.fit_transform(train[TEXT_COL])\n",
    "tfidf_valid = vectorizer.transform(valid[TEXT_COL])\n",
    "tfidf_test  = vectorizer.transform(test[TEXT_COL])\n",
    "\n",
    "tfidf_cols = [f\"tfidf_{v}\" for v in vectorizer.get_feature_names_out()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4303b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_interpretable(df):\n",
    "    all_feats = []\n",
    "    for text in df[TEXT_COL].fillna(\"\"):\n",
    "        feats = {}\n",
    "        feats.update(structural_features(text))\n",
    "        feats.update(pos_features(text))\n",
    "        all_feats.append(feats)\n",
    "    return pd.DataFrame(all_feats)\n",
    "\n",
    "interp_train = extract_interpretable(train)\n",
    "interp_valid = extract_interpretable(valid)\n",
    "interp_test  = extract_interpretable(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9162b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "bert.eval()\n",
    "\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    summed = (last_hidden_state * mask).sum(1)\n",
    "    counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts, split_name, batch_size=64, max_length=96):\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"distilbert_{split_name}.npy\")\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"loading: {cache_path}\")\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    vecs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Encoding {split_name}\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, padding=True, truncation=True,\n",
    "                        max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "        out = bert(**enc)\n",
    "        pooled = mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "        vecs.append(pooled.cpu().numpy())\n",
    "    arr = np.concatenate(vecs, axis=0)\n",
    "    np.save(cache_path, arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e2b2898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding train: 100%|██████████| 1158/1158 [00:49<00:00, 23.57it/s]\n",
      "Encoding valid: 100%|██████████| 300/300 [00:12<00:00, 23.29it/s]\n",
      "Encoding test: 100%|██████████| 368/368 [00:15<00:00, 24.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# DistilBERT\n",
    "bert_tr = encode_texts(train[TEXT_COL].fillna(\"\").tolist(), \"train\")\n",
    "bert_va = encode_texts(valid[TEXT_COL].fillna(\"\").tolist(), \"valid\")\n",
    "bert_te = encode_texts(test[TEXT_COL].fillna(\"\").tolist(),  \"test\")\n",
    "bert_cols = [f\"bert_{i}\" for i in range(bert_tr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3390df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved text feature files!\n",
      "(74111, 2854) (19156, 2854) (23540, 2854)\n"
     ]
    }
   ],
   "source": [
    "liwc_cols = train.columns[ train.columns.get_loc(\"end_time\")+1: ]\n",
    "\n",
    "def assemble(df, interp_df, tfidf_matrix, bert_arr):\n",
    "    base = df[ID_COLS].reset_index(drop=True)\n",
    "    liwc = df[liwc_cols].reset_index(drop=True)\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_cols)\n",
    "    bert_df = pd.DataFrame(bert_arr, columns=bert_cols)\n",
    "    return pd.concat([base, interp_df, liwc, tfidf_df, bert_df], axis=1)\n",
    "\n",
    "text_train = assemble(train, interp_train, tfidf_train, bert_tr)\n",
    "text_valid = assemble(valid, interp_valid, tfidf_valid, bert_va)\n",
    "text_test  = assemble(test,  interp_test,  tfidf_test,  bert_te)\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "text_train.to_csv(os.path.join(OUT_DIR, \"text_features_train.csv\"), index=False)\n",
    "text_valid.to_csv(os.path.join(OUT_DIR, \"text_features_valid.csv\"), index=False)\n",
    "text_test.to_csv(os.path.join(OUT_DIR,  \"text_features_test.csv\"),  index=False)\n",
    "\n",
    "print(\"Saved text feature files!\")\n",
    "print(text_train.shape, text_valid.shape, text_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af5251",
   "metadata": {},
   "source": [
    "### Speech-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61d11c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from transformers import Wav2Vec2FeatureExtractor, WavLMModel\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "WAV_DIR = \"./data/wav\"\n",
    "OUT_DIR = \"./task1\"\n",
    "\n",
    "ID_COLS = [\"dialog_id\", \"speaker\", \"da_tag\", \"start_time\", \"end_time\"]\n",
    "TEXT_COL = \"transcript\"   # speech 不使用，但为了对齐 ID 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eca453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = \"./data/wav\"\n",
    "\n",
    "# 全局缓存：每个 wav 文件只读一次\n",
    "wav_cache = {}\n",
    "\n",
    "def load_wav_cached(dialog_id, speaker):\n",
    "    wav_name = f\"{dialog_id}_{speaker}.wav\"\n",
    "    wav_path = os.path.join(WAV_DIR, wav_name)\n",
    "\n",
    "    if wav_path not in wav_cache:\n",
    "        wav_cache[wav_path] = parselmouth.Sound(wav_path)\n",
    "\n",
    "    return wav_cache[wav_path]\n",
    "\n",
    "def get_wav_path(dialog_id, speaker):\n",
    "    return os.path.join(WAV_DIR, f\"{dialog_id}_{speaker}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c603009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segment(wav_path, start, end):\n",
    "    snd = parselmouth.Sound(wav_path)\n",
    "    # Praat 时间是秒\n",
    "    return snd.extract_part(from_time=start, to_time=end, preserve_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c2384a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speech_features(segment: parselmouth.Sound):\n",
    "    feats = {}\n",
    "\n",
    "    dur = segment.get_total_duration()\n",
    "\n",
    "    # --- Too short to analyze ---\n",
    "    if dur < 0.06:    # 60 ms is safe\n",
    "        return {\n",
    "            \"pitch_min\": 0.0, \"pitch_max\": 0.0, \"pitch_mean\": 0.0, \"pitch_sd\": 0.0,\n",
    "            \"int_min\": 0.0, \"int_max\": 0.0, \"int_mean\": 0.0, \"int_sd\": 0.0,\n",
    "            \"jitter\": 0.0, \"shimmer\": 0.0, \"hnr_mean\": 0.0\n",
    "        }\n",
    "\n",
    "    # --- Pitch ---\n",
    "    pitch = call(segment, \"To Pitch\", 0.0, 75, 600)\n",
    "    feats[\"pitch_min\"]  = call(pitch, \"Get minimum\", 0, 0, \"Hertz\", \"Parabolic\")\n",
    "    feats[\"pitch_max\"]  = call(pitch, \"Get maximum\", 0, 0, \"Hertz\", \"Parabolic\")\n",
    "    feats[\"pitch_mean\"] = call(pitch, \"Get mean\", 0, 0, \"Hertz\")\n",
    "    feats[\"pitch_sd\"]   = call(pitch, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "\n",
    "    # --- Intensity ---\n",
    "    inte = call(segment, \"To Intensity\", 100.0, 0.0, \"yes\")\n",
    "    feats[\"int_min\"]  = call(inte, \"Get minimum\", 0, 0, \"Parabolic\")\n",
    "    feats[\"int_max\"]  = call(inte, \"Get maximum\", 0, 0, \"Parabolic\")\n",
    "    feats[\"int_mean\"] = call(inte, \"Get mean\", 0, 0, \"energy\")\n",
    "    feats[\"int_sd\"]   = call(inte, \"Get standard deviation\", 0, 0)\n",
    "\n",
    "    # --- Jitter ---\n",
    "    point_process = call(segment, \"To PointProcess (periodic, cc)\", 75, 600)\n",
    "    feats[\"jitter\"] = call(point_process, \"Get jitter (local)\",\n",
    "                           0, 0, 0.0001, 0.02, 1.3)\n",
    "\n",
    "    # --- Shimmer ---\n",
    "    feats[\"shimmer\"] = call([segment, point_process], \"Get shimmer (local)\",\n",
    "                            0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "    # --- HNR ---\n",
    "    har = call(segment, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "    feats[\"hnr_mean\"] = call(har, \"Get mean\", 0, 0)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e731f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def mfcc_librosa(wav_path, start, end, sr=16000):\n",
    "    y, sr = librosa.load(wav_path, sr=sr, offset=start, duration=end-start, mono=True)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    return {f\"mfcc_{i+1}\": float(np.mean(mfcc[i])) for i in range(13)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d9c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_for_df(df):\n",
    "    all_rows = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # ✅ 仅从缓存读取 wav（大幅减少 I/O 时间）\n",
    "            snd = load_wav_cached(row[\"dialog_id\"], row[\"speaker\"])\n",
    "\n",
    "            # ✅ 提取 partial segment\n",
    "            segment = snd.extract_part(\n",
    "                from_time=row[\"start_time\"],\n",
    "                to_time=row[\"end_time\"],\n",
    "                preserve_times=False\n",
    "            )\n",
    "\n",
    "            feats = extract_speech_features(segment)\n",
    "            \n",
    "            y = np.array(segment.values[0], dtype=float)\n",
    "            sr = int(segment.sampling_frequency)\n",
    "\n",
    "            # ---------- MFCC ----------\n",
    "            if len(y) < 400:\n",
    "                for i in range(13):\n",
    "                    feats[f\"mfcc_{i+1}\"] = 0.0\n",
    "            else:\n",
    "                # ✅ 防止 n_fft warning：使用自适应 n_fft\n",
    "                n_fft = min(2048, len(y))\n",
    "                hop_len = max(160, len(y) // 4)\n",
    "\n",
    "                mfcc = librosa.feature.mfcc(\n",
    "                    y=y,\n",
    "                    sr=sr,\n",
    "                    n_mfcc=13,\n",
    "                    n_fft=n_fft,\n",
    "                    hop_length=hop_len\n",
    "                )\n",
    "\n",
    "                for i in range(13):\n",
    "                    feats[f\"mfcc_{i+1}\"] = float(np.mean(mfcc[i]))\n",
    "\n",
    "        except Exception as e:\n",
    "            # 确保 robustness：返回 0\n",
    "            feats = {\n",
    "                \"pitch_min\": 0.0, \"pitch_max\": 0.0, \"pitch_mean\": 0.0, \"pitch_sd\": 0.0,\n",
    "                \"int_min\": 0.0, \"int_max\": 0.0, \"int_mean\": 0.0, \"int_sd\": 0.0,\n",
    "                \"jitter\": 0.0, \"shimmer\": 0.0, \"hnr_mean\": 0.0\n",
    "            }\n",
    "            for i in range(13):\n",
    "                feats[f\"mfcc_{i+1}\"] = 0.0\n",
    "\n",
    "        all_rows.append(feats)\n",
    "\n",
    "        # Optional: progress indicator\n",
    "        if idx % 2000 == 0:\n",
    "            print(f\"Processed {idx}/{len(df)} segments\")\n",
    "\n",
    "    return pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18304b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/74111 segments\n",
      "Processed 2000/74111 segments\n",
      "Processed 4000/74111 segments\n",
      "Processed 6000/74111 segments\n",
      "Processed 8000/74111 segments\n",
      "Processed 10000/74111 segments\n",
      "Processed 12000/74111 segments\n",
      "Processed 14000/74111 segments\n",
      "Processed 16000/74111 segments\n",
      "Processed 18000/74111 segments\n",
      "Processed 20000/74111 segments\n",
      "Processed 22000/74111 segments\n",
      "Processed 24000/74111 segments\n",
      "Processed 26000/74111 segments\n",
      "Processed 28000/74111 segments\n",
      "Processed 30000/74111 segments\n",
      "Processed 32000/74111 segments\n",
      "Processed 34000/74111 segments\n",
      "Processed 36000/74111 segments\n",
      "Processed 38000/74111 segments\n",
      "Processed 40000/74111 segments\n",
      "Processed 42000/74111 segments\n",
      "Processed 44000/74111 segments\n",
      "Processed 46000/74111 segments\n",
      "Processed 48000/74111 segments\n",
      "Processed 50000/74111 segments\n",
      "Processed 52000/74111 segments\n",
      "Processed 54000/74111 segments\n",
      "Processed 56000/74111 segments\n",
      "Processed 58000/74111 segments\n",
      "Processed 60000/74111 segments\n",
      "Processed 62000/74111 segments\n",
      "Processed 64000/74111 segments\n",
      "Processed 66000/74111 segments\n",
      "Processed 68000/74111 segments\n",
      "Processed 70000/74111 segments\n",
      "Processed 72000/74111 segments\n",
      "Processed 74000/74111 segments\n",
      "Processed 0/19156 segments\n",
      "Processed 2000/19156 segments\n",
      "Processed 4000/19156 segments\n",
      "Processed 6000/19156 segments\n",
      "Processed 8000/19156 segments\n",
      "Processed 10000/19156 segments\n",
      "Processed 12000/19156 segments\n",
      "Processed 14000/19156 segments\n",
      "Processed 16000/19156 segments\n",
      "Processed 18000/19156 segments\n",
      "Processed 0/23540 segments\n",
      "Processed 2000/23540 segments\n",
      "Processed 4000/23540 segments\n",
      "Processed 6000/23540 segments\n",
      "Processed 8000/23540 segments\n",
      "Processed 10000/23540 segments\n",
      "Processed 12000/23540 segments\n",
      "Processed 14000/23540 segments\n",
      "Processed 16000/23540 segments\n",
      "Processed 18000/23540 segments\n",
      "Processed 20000/23540 segments\n",
      "Processed 22000/23540 segments\n",
      "Speech features saved!\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "valid = pd.read_csv(f\"{DATA_DIR}/valid.csv\")\n",
    "test  = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
    "\n",
    "speech_train = extract_for_df(train)\n",
    "speech_valid = extract_for_df(valid)\n",
    "speech_test  = extract_for_df(test)\n",
    "\n",
    "speech_train = pd.concat([train[ID_COLS].reset_index(drop=True), speech_train], axis=1)\n",
    "speech_valid = pd.concat([valid[ID_COLS].reset_index(drop=True), speech_valid], axis=1)\n",
    "speech_test  = pd.concat([test[ID_COLS].reset_index(drop=True), speech_test], axis=1)\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "speech_train.to_csv(f\"{OUT_DIR}/speech_features_train.csv\", index=False)\n",
    "speech_valid.to_csv(f\"{OUT_DIR}/speech_features_valid.csv\", index=False)\n",
    "speech_test.to_csv(f\"{OUT_DIR}/speech_features_test.csv\", index=False)\n",
    "\n",
    "print(\"Speech features saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c0d8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/wavlm-base-plus\"\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor, WavLMModel\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = WavLMModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_extract_wavlm(wav_info_list, batch_size=8, sr=16000):\n",
    "    \"\"\"\n",
    "    wav_info_list: list of (wav_path, start_time, end_time)\n",
    "    return: np.ndarray [N, 768]\n",
    "    \"\"\"\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(wav_info_list), batch_size), desc=\"Encoding WavLM batched\"):\n",
    "        batch_waveforms = []\n",
    "\n",
    "        # ---- 批量加载音频 ----\n",
    "        for j in range(i, min(i + batch_size, len(wav_info_list))):\n",
    "            wav_path, start, end = wav_info_list[j]\n",
    "            duration = max(0.05, end - start)\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_path, sr=sr, offset=start, duration=duration, mono=True)\n",
    "            except Exception:\n",
    "                y = np.zeros(160)\n",
    "            if len(y) < 160:\n",
    "                y = np.zeros(160)\n",
    "            batch_waveforms.append(y)\n",
    "\n",
    "        # ---- 提取特征 ----\n",
    "        inputs = feature_extractor(batch_waveforms, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        all_embs.append(emb)\n",
    "\n",
    "    return np.concatenate(all_embs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f151db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding WavLM batched:   0%|          | 0/2316 [00:00<?, ?it/s]/Users/kenzieluo/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Encoding WavLM batched:   0%|          | 8/2316 [01:04<5:11:54,  8.11s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     np.save(cache_path, embs)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embs\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m wavlm_train = \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeech_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m wavlm_valid = process_split(speech_valid, \u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m wavlm_test  = process_split(speech_test, \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mprocess_split\u001b[39m\u001b[34m(df, split_name)\u001b[39m\n\u001b[32m     14\u001b[39m wav_info_list = [\n\u001b[32m     15\u001b[39m     (os.path.join(WAV_DIR, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mdialog_id\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mspeaker\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.wav\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     16\u001b[39m      row[\u001b[33m\"\u001b[39m\u001b[33mstart_time\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mend_time\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows()\n\u001b[32m     18\u001b[39m ]\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 批量提取\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m embs = \u001b[43mbatch_extract_wavlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_info_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m np.save(cache_path, embs)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mbatch_extract_wavlm\u001b[39m\u001b[34m(wav_info_list, batch_size, sr)\u001b[39m\n\u001b[32m     32\u001b[39m inputs = feature_extractor(batch_waveforms, sampling_rate=sr, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m emb = outputs.last_hidden_state.mean(dim=\u001b[32m1\u001b[39m).cpu().numpy()\n\u001b[32m     37\u001b[39m all_embs.append(emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:1085\u001b[39m, in \u001b[36mWavLMModel.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1080\u001b[39m hidden_states, extract_features = \u001b[38;5;28mself\u001b[39m.feature_projection(extract_features)\n\u001b[32m   1081\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m._mask_hidden_states(\n\u001b[32m   1082\u001b[39m     hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask\n\u001b[32m   1083\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m hidden_states = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.adapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:422\u001b[39m, in \u001b[36mWavLMEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    419\u001b[39m skip_the_layer = \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;129;01mand\u001b[39;00m i > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (dropout_probability < \u001b[38;5;28mself\u001b[39m.config.layerdrop)\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_the_layer \u001b[38;5;129;01mor\u001b[39;00m synced_gpus:\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# under fsdp or deepspeed zero3 all gpus must run in sync\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     layer_outputs = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     hidden_states, position_bias = layer_outputs[:\u001b[32m2\u001b[39m]\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:316\u001b[39m, in \u001b[36mWavLMEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, output_attentions, index)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m, position_bias=\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m, index=\u001b[32m0\u001b[39m):\n\u001b[32m    315\u001b[39m     attn_residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     hidden_states, attn_weights, position_bias = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    324\u001b[39m     hidden_states = attn_residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:182\u001b[39m, in \u001b[36mWavLMAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, output_attentions, index)\u001b[39m\n\u001b[32m    179\u001b[39m gated_position_bias = gate_output.view(bsz * \u001b[38;5;28mself\u001b[39m.num_heads, -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) * position_bias\n\u001b[32m    180\u001b[39m gated_position_bias = gated_position_bias.view((-\u001b[32m1\u001b[39m, tgt_len, tgt_len))\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m attn_output, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_multi_head_self_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgated_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights, position_bias\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/transformers/models/wavlm/modeling_wavlm.py:206\u001b[39m, in \u001b[36mWavLMAttention.torch_multi_head_self_attention\u001b[39m\u001b[34m(self, hidden_states, attention_mask, gated_position_bias, output_attentions)\u001b[39m\n\u001b[32m    202\u001b[39m add_zero_attn = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# PyTorch 1.3.0 has F.multi_head_attention_forward defined\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# so no problem with backwards compatibility\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m attn_output, attn_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgated_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_separate_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# [Seq_Len, Batch Size, ...] -> [Batch Size, Seq_Len, ...]\u001b[39;00m\n\u001b[32m    231\u001b[39m attn_output = attn_output.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/file/anaconda/anaconda3/envs/slp/lib/python3.12/site-packages/torch/nn/functional.py:6487\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6484\u001b[39m k = k.view(bsz, num_heads, src_len, head_dim)\n\u001b[32m   6485\u001b[39m v = v.view(bsz, num_heads, src_len, head_dim)\n\u001b[32m-> \u001b[39m\u001b[32m6487\u001b[39m attn_output = \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\n\u001b[32m   6489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6490\u001b[39m attn_output = (\n\u001b[32m   6491\u001b[39m     attn_output.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m).contiguous().view(bsz * tgt_len, embed_dim)\n\u001b[32m   6492\u001b[39m )\n\u001b[32m   6494\u001b[39m attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "speech_train = pd.read_csv(os.path.join(OUT_DIR, \"speech_features_train.csv\"))\n",
    "speech_valid = pd.read_csv(os.path.join(OUT_DIR, \"speech_features_valid.csv\"))\n",
    "speech_test  = pd.read_csv(os.path.join(OUT_DIR, \"speech_features_test.csv\"))\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Compute & cache embeddings\n",
    "# -----------------------------\n",
    "def process_split(df, split_name):\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"wavlm_{split_name}.npy\")\n",
    "    if os.path.exists(cache_path):\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    # 生成所有 (wav_path, start_time, end_time)\n",
    "    wav_info_list = [\n",
    "        (os.path.join(WAV_DIR, f\"{row['dialog_id']}_{row['speaker']}.wav\"),\n",
    "         row[\"start_time\"], row[\"end_time\"])\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # 批量提取\n",
    "    embs = batch_extract_wavlm(wav_info_list, batch_size=32)\n",
    "    np.save(cache_path, embs)\n",
    "    return embs\n",
    "\n",
    "wavlm_train = process_split(speech_train, \"train\")\n",
    "wavlm_valid = process_split(speech_valid, \"valid\")\n",
    "wavlm_test  = process_split(speech_test, \"test\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# -----------------------------\n",
    "# 5. Merge with handcrafted features\n",
    "# -----------------------------\n",
    "cols = [f\"wavlm_{i}\" for i in range(wavlm_train.shape[1])]\n",
    "wavlm_df_train = pd.DataFrame(wavlm_train, columns=cols)\n",
    "wavlm_df_valid = pd.DataFrame(wavlm_valid, columns=cols)\n",
    "wavlm_df_test  = pd.DataFrame(wavlm_test, columns=cols)\n",
    "\n",
    "speech_train_wavlm = pd.concat([speech_train.reset_index(drop=True), wavlm_df_train], axis=1)\n",
    "speech_valid_wavlm = pd.concat([speech_valid.reset_index(drop=True), wavlm_df_valid], axis=1)\n",
    "speech_test_wavlm  = pd.concat([speech_test.reset_index(drop=True), wavlm_df_test], axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save enhanced feature CSVs\n",
    "# -----------------------------\n",
    "speech_train_wavlm.to_csv(os.path.join(OUT_DIR, \"speech_features_train_wavlm.csv\"), index=False)\n",
    "speech_valid_wavlm.to_csv(os.path.join(OUT_DIR, \"speech_features_valid_wavlm.csv\"), index=False)\n",
    "speech_test_wavlm.to_csv(os.path.join(OUT_DIR, \"speech_features_test_wavlm.csv\"), index=False)\n",
    "\n",
    "print(\"✅ Enhanced speech features (with WavLM) saved!\")\n",
    "print(\"Train shape:\", speech_train_wavlm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b87bf6",
   "metadata": {},
   "source": [
    "### a. Describe your custom feature sets (text-based features and speech-based\n",
    "features), the reasoning behind choosing them and the techniques used to\n",
    "extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa1943a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74111, 2854) (19156, 2854) (23540, 2854)\n",
      "Text feature columns: 2854\n",
      "['dialog_id', 'speaker', 'da_tag', 'start_time', 'end_time', 'len_chars', 'len_tokens', 'num_punct', 'ends_question', 'pos_prop_NOUN', 'pos_prop_VERB', 'pos_prop_ADJ', 'pos_prop_ADV', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body', 'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'tfidf_able', 'tfidf_able to', 'tfidf_about', 'tfidf_about it', 'tfidf_about that', 'tfidf_about the', 'tfidf_about two', 'tfidf_about uh', 'tfidf_absolutely', 'tfidf_across', 'tfidf_across the', 'tfidf_actually', 'tfidf_afford', 'tfidf_after', 'tfidf_again', 'tfidf_against', 'tfidf_age', 'tfidf_ago', 'tfidf_agree', 'tfidf_agree with', 'tfidf_ahead', 'tfidf_air', 'tfidf_air pollution', 'tfidf_all', 'tfidf_all of', 'tfidf_all over', 'tfidf_all right', 'tfidf_all that', 'tfidf_all the', 'tfidf_all these', 'tfidf_almost', 'tfidf_alone', 'tfidf_along', 'tfidf_already', 'tfidf_also', 'tfidf_although', 'tfidf_always', 'tfidf_am', 'tfidf_america', 'tfidf_american', 'tfidf_amount', 'tfidf_amount of', 'tfidf_an', 'tfidf_an hour', 'tfidf_an interesting', 'tfidf_and', 'tfidf_and all', 'tfidf_and also', 'tfidf_and and', 'tfidf_and but', 'tfidf_and can', 'tfidf_and do', 'tfidf_and don', 'tfidf_and everything', 'tfidf_and get', 'tfidf_and guess', 'tfidf_and half', 'tfidf_and have', 'tfidf_and he', 'tfidf_and how', 'tfidf_and if', 'tfidf_and in', 'tfidf_and it', 'tfidf_and just', 'tfidf_and know', 'tfidf_and like', 'tfidf_and lot', 'tfidf_and maybe', 'tfidf_and mean', 'tfidf_and more', 'tfidf_and my', 'tfidf_and not', 'tfidf_and now', 'tfidf_and of', 'tfidf_and one', 'tfidf_and pause', 'tfidf_and really', 'tfidf_and say', 'tfidf_and she', 'tfidf_and so', 'tfidf_and some', 'tfidf_and sometimes', 'tfidf_and stuff', 'tfidf_and that', 'tfidf_and the', 'tfidf_and then', 'tfidf_and there', 'tfidf_and they', 'tfidf_and things', 'tfidf_and think', 'tfidf_and this', 'tfidf_and uh', 'tfidf_and um', 'tfidf_and ve', 'tfidf_and was', 'tfidf_and we', 'tfidf_and what', 'tfidf_and when', 'tfidf_and you', 'tfidf_another', 'tfidf_another one', 'tfidf_any', 'tfidf_any of', 'tfidf_anybody', 'tfidf_anymore', 'tfidf_anything', 'tfidf_anything like', 'tfidf_anyway', 'tfidf_apartment', 'tfidf_apparently', 'tfidf_are', 'tfidf_are are', 'tfidf_are going', 'tfidf_are in', 'tfidf_are just', 'tfidf_are not', 'tfidf_are really', 'tfidf_are the', 'tfidf_are they', 'tfidf_are very', 'tfidf_are you', 'tfidf_area', 'tfidf_areas', 'tfidf_aren', 'tfidf_around', 'tfidf_around here', 'tfidf_around the', 'tfidf_as', 'tfidf_as as', 'tfidf_as far', 'tfidf_as much', 'tfidf_as the', 'tfidf_as they', 'tfidf_as uh', 'tfidf_as well', 'tfidf_as you', 'tfidf_ask', 'tfidf_asked', 'tfidf_at', 'tfidf_at all', 'tfidf_at at', 'tfidf_at home', 'tfidf_at it', 'tfidf_at least', 'tfidf_at night', 'tfidf_at that', 'tfidf_at the', 'tfidf_at work', 'tfidf_available', 'tfidf_aware', 'tfidf_away', 'tfidf_away from', 'tfidf_awful', 'tfidf_baby', 'tfidf_back', 'tfidf_back to', 'tfidf_bad', 'tfidf_baseball', 'tfidf_basic', 'tfidf_basically', 'tfidf_be', 'tfidf_be able', 'tfidf_be done', 'tfidf_be good', 'tfidf_be in', 'tfidf_be the', 'tfidf_because', 'tfidf_because he', 'tfidf_because it', 'tfidf_because my', 'tfidf_because of', 'tfidf_because that', 'tfidf_because the', 'tfidf_because there', 'tfidf_because they', 'tfidf_because uh', 'tfidf_because we', 'tfidf_because you', 'tfidf_become', 'tfidf_been', 'tfidf_been in', 'tfidf_been to', 'tfidf_before', 'tfidf_before they', 'tfidf_being', 'tfidf_believe', 'tfidf_believe that', 'tfidf_benefits', 'tfidf_best', 'tfidf_bet', 'tfidf_better', 'tfidf_between', 'tfidf_big', 'tfidf_biggest', 'tfidf_bill', 'tfidf_bit', 'tfidf_bit of', 'tfidf_black', 'tfidf_book', 'tfidf_books', 'tfidf_both', 'tfidf_bought', 'tfidf_boy', 'tfidf_break', 'tfidf_bring', 'tfidf_brother', 'tfidf_brought', 'tfidf_budget', 'tfidf_building', 'tfidf_built', 'tfidf_bush', 'tfidf_business', 'tfidf_but', 'tfidf_but but', 'tfidf_but do', 'tfidf_but don', 'tfidf_but he', 'tfidf_but if', 'tfidf_but it', 'tfidf_but just', 'tfidf_but mean', 'tfidf_but not', 'tfidf_but now', 'tfidf_but still', 'tfidf_but that', 'tfidf_but the', 'tfidf_but then', 'tfidf_but there', 'tfidf_but they', 'tfidf_but think', 'tfidf_but uh', 'tfidf_but um', 'tfidf_but ve', 'tfidf_but we', 'tfidf_but you', 'tfidf_buy', 'tfidf_buying', 'tfidf_by', 'tfidf_by the', 'tfidf_bye', 'tfidf_bye bye', 'tfidf_california', 'tfidf_call', 'tfidf_call it', 'tfidf_called', 'tfidf_came', 'tfidf_camping', 'tfidf_can', 'tfidf_can be', 'tfidf_can do', 'tfidf_can get', 'tfidf_can just', 'tfidf_can really', 'tfidf_can remember', 'tfidf_can see', 'tfidf_can take', 'tfidf_capital', 'tfidf_car', 'tfidf_card', 'tfidf_cards', 'tfidf_care', 'tfidf_care of', 'tfidf_cars', 'tfidf_case', 'tfidf_cases', 'tfidf_cat', 'tfidf_catch', 'tfidf_cats', 'tfidf_caught', 'tfidf_cause', 'tfidf_certain', 'tfidf_certainly', 'tfidf_chance', 'tfidf_change', 'tfidf_changed', 'tfidf_changes', 'tfidf_charge', 'tfidf_check', 'tfidf_child', 'tfidf_children', 'tfidf_choice', 'tfidf_church', 'tfidf_city', 'tfidf_class', 'tfidf_clock', 'tfidf_close', 'tfidf_cold', 'tfidf_college', 'tfidf_come', 'tfidf_come in', 'tfidf_come out', 'tfidf_come to', 'tfidf_comes', 'tfidf_coming', 'tfidf_community', 'tfidf_companies', 'tfidf_company', 'tfidf_completely', 'tfidf_computer', 'tfidf_concerned', 'tfidf_consider', 'tfidf_control', 'tfidf_cook', 'tfidf_cost', 'tfidf_could', 'tfidf_could be', 'tfidf_could do', 'tfidf_couldn', 'tfidf_countries', 'tfidf_country', 'tfidf_couple', 'tfidf_couple of', 'tfidf_course', 'tfidf_cowboys', 'tfidf_credit', 'tfidf_credit cards', 'tfidf_crime', 'tfidf_cut', 'tfidf_dad', 'tfidf_dallas', 'tfidf_daughter', 'tfidf_day', 'tfidf_days', 'tfidf_deal', 'tfidf_deal with', 'tfidf_death', 'tfidf_decided', 'tfidf_definitely', 'tfidf_depends', 'tfidf_did', 'tfidf_did it', 'tfidf_did you', 'tfidf_didn', 'tfidf_didn have', 'tfidf_didn know', 'tfidf_difference', 'tfidf_different', 'tfidf_difficult', 'tfidf_dinner', 'tfidf_do', 'tfidf_do have', 'tfidf_do is', 'tfidf_do it', 'tfidf_do lot', 'tfidf_do not', 'tfidf_do something', 'tfidf_do that', 'tfidf_do the', 'tfidf_do they', 'tfidf_do too', 'tfidf_do with', 'tfidf_do you', 'tfidf_doctor', 'tfidf_does', 'tfidf_does it', 'tfidf_doesn', 'tfidf_doesn have', 'tfidf_dog', 'tfidf_dogs', 'tfidf_doing', 'tfidf_doing it', 'tfidf_doing that', 'tfidf_dollar', 'tfidf_dollars', 'tfidf_don', 'tfidf_don do', 'tfidf_don don', 'tfidf_don even', 'tfidf_don get', 'tfidf_don have', 'tfidf_don know', 'tfidf_don like', 'tfidf_don really', 'tfidf_don see', 'tfidf_don think', 'tfidf_don want', 'tfidf_don you', 'tfidf_done', 'tfidf_down', 'tfidf_down here', 'tfidf_down there', 'tfidf_down to', 'tfidf_dress', 'tfidf_drive', 'tfidf_driving', 'tfidf_drug', 'tfidf_drugs', 'tfidf_during', 'tfidf_during the', 'tfidf_each', 'tfidf_early', 'tfidf_easier', 'tfidf_east', 'tfidf_easy', 'tfidf_easy to', 'tfidf_eat', 'tfidf_economy', 'tfidf_education', 'tfidf_eight', 'tfidf_eighty', 'tfidf_either', 'tfidf_else', 'tfidf_end', 'tfidf_end of', 'tfidf_end up', 'tfidf_ended', 'tfidf_ended up', 'tfidf_enjoy', 'tfidf_enjoy it', 'tfidf_enjoyed', 'tfidf_enough', 'tfidf_enough to', 'tfidf_especially', 'tfidf_even', 'tfidf_even if', 'tfidf_even though', 'tfidf_ever', 'tfidf_every', 'tfidf_every time', 'tfidf_everybody', 'tfidf_everyone', 'tfidf_everything', 'tfidf_exactly', 'tfidf_example', 'tfidf_excellent', 'tfidf_except', 'tfidf_exercise', 'tfidf_expensive', 'tfidf_experience', 'tfidf_extra', 'tfidf_fact', 'tfidf_fact that', 'tfidf_fairly', 'tfidf_familiar', 'tfidf_families', 'tfidf_family', 'tfidf_fan', 'tfidf_far', 'tfidf_far as', 'tfidf_father', 'tfidf_favorite', 'tfidf_federal', 'tfidf_feel', 'tfidf_feel like', 'tfidf_feel that', 'tfidf_feet', 'tfidf_few', 'tfidf_few years', 'tfidf_field', 'tfidf_fifteen', 'tfidf_fifty', 'tfidf_figure', 'tfidf_finally', 'tfidf_find', 'tfidf_find that', 'tfidf_fine', 'tfidf_first', 'tfidf_fish', 'tfidf_fishing', 'tfidf_five', 'tfidf_five years', 'tfidf_fixed', 'tfidf_flowers', 'tfidf_food', 'tfidf_football', 'tfidf_for', 'tfidf_for about', 'tfidf_for example', 'tfidf_for for', 'tfidf_for it', 'tfidf_for me', 'tfidf_for my', 'tfidf_for that', 'tfidf_for the', 'tfidf_for them', 'tfidf_for uh', 'tfidf_for us', 'tfidf_for while', 'tfidf_for you', 'tfidf_forth', 'tfidf_forty', 'tfidf_found', 'tfidf_four', 'tfidf_free', 'tfidf_friend', 'tfidf_friends', 'tfidf_from', 'tfidf_from the', 'tfidf_front', 'tfidf_full', 'tfidf_fun', 'tfidf_funny', 'tfidf_game', 'tfidf_games', 'tfidf_garden', 'tfidf_gave', 'tfidf_general', 'tfidf_generally', 'tfidf_get', 'tfidf_get in', 'tfidf_get into', 'tfidf_get it', 'tfidf_get out', 'tfidf_get the', 'tfidf_get to', 'tfidf_get up', 'tfidf_gets', 'tfidf_getting', 'tfidf_give', 'tfidf_give you', 'tfidf_given', 'tfidf_gives', 'tfidf_giving', 'tfidf_glad', 'tfidf_go', 'tfidf_go back', 'tfidf_go on', 'tfidf_go out', 'tfidf_go to', 'tfidf_god', 'tfidf_goes', 'tfidf_going', 'tfidf_going on', 'tfidf_going to', 'tfidf_gone', 'tfidf_good', 'tfidf_goodness', 'tfidf_gosh', 'tfidf_got', 'tfidf_got some', 'tfidf_got the', 'tfidf_got to', 'tfidf_gotten', 'tfidf_government', 'tfidf_grade', 'tfidf_grandmother', 'tfidf_great', 'tfidf_grew', 'tfidf_grew up', 'tfidf_group', 'tfidf_groups', 'tfidf_grow', 'tfidf_growing', 'tfidf_guess', 'tfidf_guess it', 'tfidf_guess that', 'tfidf_guess they', 'tfidf_guess we', 'tfidf_guess you', 'tfidf_gun', 'tfidf_guy', 'tfidf_guys', 'tfidf_had', 'tfidf_had an', 'tfidf_had one', 'tfidf_had some', 'tfidf_had the', 'tfidf_had to', 'tfidf_had uh', 'tfidf_hadn', 'tfidf_half', 'tfidf_hand', 'tfidf_happen', 'tfidf_happened', 'tfidf_happening', 'tfidf_happens', 'tfidf_hard', 'tfidf_hard to', 'tfidf_has', 'tfidf_has been', 'tfidf_has to', 'tfidf_hate', 'tfidf_have', 'tfidf_have an', 'tfidf_have any', 'tfidf_have been', 'tfidf_have had', 'tfidf_have it', 'tfidf_have lot', 'tfidf_have one', 'tfidf_have some', 'tfidf_have that', 'tfidf_have the', 'tfidf_have them', 'tfidf_have to', 'tfidf_have two', 'tfidf_have uh', 'tfidf_have you', 'tfidf_haven', 'tfidf_haven been', 'tfidf_having', 'tfidf_he', 'tfidf_he got', 'tfidf_he had', 'tfidf_he has', 'tfidf_he he', 'tfidf_he is', 'tfidf_he just', 'tfidf_he said', 'tfidf_he was', 'tfidf_health', 'tfidf_health care', 'tfidf_hear', 'tfidf_heard', 'tfidf_heard of', 'tfidf_heavy', 'tfidf_help', 'tfidf_her', 'tfidf_here', 'tfidf_here in', 'tfidf_hey', 'tfidf_high', 'tfidf_high school', 'tfidf_him', 'tfidf_his', 'tfidf_hit', 'tfidf_hold', 'tfidf_home', 'tfidf_home and', 'tfidf_homes', 'tfidf_hope', 'tfidf_hot', 'tfidf_hour', 'tfidf_hours', 'tfidf_house', 'tfidf_how', 'tfidf_how many', 'tfidf_how much', 'tfidf_how they', 'tfidf_how to', 'tfidf_huh', 'tfidf_huh uh', 'tfidf_hundred', 'tfidf_hundred and', 'tfidf_hundred dollars', 'tfidf_hurt', 'tfidf_husband', 'tfidf_husband and', 'tfidf_idea', 'tfidf_if', 'tfidf_if he', 'tfidf_if if', 'tfidf_if it', 'tfidf_if the', 'tfidf_if there', 'tfidf_if they', 'tfidf_if we', 'tfidf_if you', 'tfidf_imagine', 'tfidf_important', 'tfidf_in', 'tfidf_in an', 'tfidf_in and', 'tfidf_in dallas', 'tfidf_in fact', 'tfidf_in in', 'tfidf_in it', 'tfidf_in my', 'tfidf_in one', 'tfidf_in our', 'tfidf_in plano', 'tfidf_in school', 'tfidf_in some', 'tfidf_in terms', 'tfidf_in texas', 'tfidf_in that', 'tfidf_in the', 'tfidf_in their', 'tfidf_in there', 'tfidf_in this', 'tfidf_in uh', 'tfidf_in you', 'tfidf_in your', 'tfidf_income', 'tfidf_inside', 'tfidf_instead', 'tfidf_instead of', 'tfidf_insurance', 'tfidf_interest', 'tfidf_interested', 'tfidf_interested in', 'tfidf_interesting', 'tfidf_into', 'tfidf_into the', 'tfidf_involved', 'tfidf_involved in', 'tfidf_is', 'tfidf_is going', 'tfidf_is good', 'tfidf_is in', 'tfidf_is is', 'tfidf_is it', 'tfidf_is just', 'tfidf_is like', 'tfidf_is not', 'tfidf_is really', 'tfidf_is so', 'tfidf_is that', 'tfidf_is the', 'tfidf_is to', 'tfidf_is uh', 'tfidf_is what', 'tfidf_is you', 'tfidf_isn', 'tfidf_issue', 'tfidf_it', 'tfidf_it all', 'tfidf_it an', 'tfidf_it and', 'tfidf_it as', 'tfidf_it at', 'tfidf_it because', 'tfidf_it been', 'tfidf_it can', 'tfidf_it does', 'tfidf_it doesn', 'tfidf_it for', 'tfidf_it gets', 'tfidf_it going', 'tfidf_it good', 'tfidf_it got', 'tfidf_it had', 'tfidf_it hard', 'tfidf_it has', 'tfidf_it in', 'tfidf_it is', 'tfidf_it it', 'tfidf_it just', 'tfidf_it kind', 'tfidf_it like', 'tfidf_it little', 'tfidf_it ll', 'tfidf_it lot', 'tfidf_it more', 'tfidf_it nice', 'tfidf_it not', 'tfidf_it on', 'tfidf_it out', 'tfidf_it pretty', 'tfidf_it probably', 'tfidf_it real', 'tfidf_it really', 'tfidf_it seems', 'tfidf_it so', 'tfidf_it sounds', 'tfidf_it still', 'tfidf_it that', 'tfidf_it the', 'tfidf_it to', 'tfidf_it too', 'tfidf_it uh', 'tfidf_it up', 'tfidf_it very', 'tfidf_it was', 'tfidf_it wasn', 'tfidf_it will', 'tfidf_it would', 'tfidf_it you', 'tfidf_its', 'tfidf_job', 'tfidf_jobs', 'tfidf_jury', 'tfidf_just', 'tfidf_just about', 'tfidf_just because', 'tfidf_just don', 'tfidf_just got', 'tfidf_just have', 'tfidf_just just', 'tfidf_just kind', 'tfidf_just like', 'tfidf_just that', 'tfidf_just the', 'tfidf_just to', 'tfidf_just uh', 'tfidf_just you', 'tfidf_keep', 'tfidf_kept', 'tfidf_kid', 'tfidf_kids', 'tfidf_kids are', 'tfidf_kill', 'tfidf_kind', 'tfidf_kind of', 'tfidf_kinds', 'tfidf_kinds of', 'tfidf_knew', 'tfidf_know', 'tfidf_know about', 'tfidf_know and', 'tfidf_know because', 'tfidf_know don', 'tfidf_know for', 'tfidf_know he', 'tfidf_know how', 'tfidf_know if', 'tfidf_know in', 'tfidf_know it', 'tfidf_know just', 'tfidf_know like', 'tfidf_know lot', 'tfidf_know my', 'tfidf_know some', 'tfidf_know that', 'tfidf_know the', 'tfidf_know there', 'tfidf_know they', 'tfidf_know think', 'tfidf_know to', 'tfidf_know uh', 'tfidf_know we', 'tfidf_know what', 'tfidf_know when', 'tfidf_know where', 'tfidf_know you', 'tfidf_lake', 'tfidf_large', 'tfidf_last', 'tfidf_last year', 'tfidf_late', 'tfidf_lately', 'tfidf_later', 'tfidf_laughter', 'tfidf_law', 'tfidf_learn', 'tfidf_least', 'tfidf_leave', 'tfidf_left', 'tfidf_less', 'tfidf_let', 'tfidf_level', 'tfidf_life', 'tfidf_like', 'tfidf_like if', 'tfidf_like in', 'tfidf_like it', 'tfidf_like said', 'tfidf_like that', 'tfidf_like the', 'tfidf_like they', 'tfidf_like to', 'tfidf_like uh', 'tfidf_like we', 'tfidf_like you', 'tfidf_liked', 'tfidf_likes', 'tfidf_line', 'tfidf_listen', 'tfidf_listen to', 'tfidf_little', 'tfidf_little bit', 'tfidf_little more', 'tfidf_live', 'tfidf_live in', 'tfidf_lived', 'tfidf_lived in', 'tfidf_lives', 'tfidf_living', 'tfidf_ll', 'tfidf_ll be', 'tfidf_ll go', 'tfidf_ll have', 'tfidf_local', 'tfidf_long', 'tfidf_long time', 'tfidf_longer', 'tfidf_look', 'tfidf_look at', 'tfidf_looked', 'tfidf_looking', 'tfidf_looking at', 'tfidf_looks', 'tfidf_lost', 'tfidf_lot', 'tfidf_lot more', 'tfidf_lot of', 'tfidf_lots', 'tfidf_lots of', 'tfidf_love', 'tfidf_love to', 'tfidf_made', 'tfidf_main', 'tfidf_major', 'tfidf_make', 'tfidf_make it', 'tfidf_makes', 'tfidf_making', 'tfidf_man', 'tfidf_many', 'tfidf_many people', 'tfidf_market', 'tfidf_married', 'tfidf_matter', 'tfidf_matter of', 'tfidf_may', 'tfidf_may be', 'tfidf_maybe', 'tfidf_me', 'tfidf_me that', 'tfidf_me to', 'tfidf_mean', 'tfidf_mean it', 'tfidf_mean that', 'tfidf_mean the', 'tfidf_mean there', 'tfidf_mean they', 'tfidf_mean we', 'tfidf_mean you', 'tfidf_medical', 'tfidf_men', 'tfidf_metric', 'tfidf_middle', 'tfidf_might', 'tfidf_might be', 'tfidf_miles', 'tfidf_military', 'tfidf_mind', 'tfidf_mine', 'tfidf_minutes', 'tfidf_mom', 'tfidf_money', 'tfidf_money to', 'tfidf_month', 'tfidf_months', 'tfidf_more', 'tfidf_more of', 'tfidf_more than', 'tfidf_morning', 'tfidf_most', 'tfidf_most of', 'tfidf_mostly', 'tfidf_mother', 'tfidf_move', 'tfidf_moved', 'tfidf_movie', 'tfidf_movies', 'tfidf_much', 'tfidf_much as', 'tfidf_much of', 'tfidf_music', 'tfidf_must', 'tfidf_my', 'tfidf_my dad', 'tfidf_my family', 'tfidf_my father', 'tfidf_my husband', 'tfidf_my kids', 'tfidf_my mother', 'tfidf_my my', 'tfidf_my own', 'tfidf_my parents', 'tfidf_my son', 'tfidf_my uh', 'tfidf_my wife', 'tfidf_myself', 'tfidf_name', 'tfidf_national', 'tfidf_neat', 'tfidf_necessarily', 'tfidf_need', 'tfidf_need to', 'tfidf_needed', 'tfidf_needs', 'tfidf_needs to', 'tfidf_never', 'tfidf_new', 'tfidf_news', 'tfidf_newspaper', 'tfidf_next', 'tfidf_nice', 'tfidf_nice talking', 'tfidf_nice to', 'tfidf_night', 'tfidf_nine', 'tfidf_nineteen', 'tfidf_ninety', 'tfidf_no', 'tfidf_nobody', 'tfidf_noise', 'tfidf_north', 'tfidf_not', 'tfidf_not going', 'tfidf_not in', 'tfidf_not really', 'tfidf_not sure', 'tfidf_not that', 'tfidf_not the', 'tfidf_not to', 'tfidf_not too', 'tfidf_nothing', 'tfidf_now', 'tfidf_now that', 'tfidf_now they', 'tfidf_now we', 'tfidf_number', 'tfidf_number of', 'tfidf_nursing', 'tfidf_nursing home', 'tfidf_obviously', 'tfidf_of', 'tfidf_of all', 'tfidf_of an', 'tfidf_of course', 'tfidf_of different', 'tfidf_of fact', 'tfidf_of fun', 'tfidf_of her', 'tfidf_of it', 'tfidf_of like', 'tfidf_of money', 'tfidf_of my', 'tfidf_of of', 'tfidf_of our', 'tfidf_of people', 'tfidf_of stuff', 'tfidf_of that', 'tfidf_of the', 'tfidf_of their', 'tfidf_of them', 'tfidf_of these', 'tfidf_of thing', 'tfidf_of things', 'tfidf_of those', 'tfidf_of time', 'tfidf_of times', 'tfidf_of uh', 'tfidf_of us', 'tfidf_of what', 'tfidf_of years', 'tfidf_of you', 'tfidf_of your', 'tfidf_off', 'tfidf_off the', 'tfidf_office', 'tfidf_often', 'tfidf_oh', 'tfidf_oh it', 'tfidf_oh know', 'tfidf_oh my', 'tfidf_oh no', 'tfidf_oh okay', 'tfidf_oh really', 'tfidf_oh that', 'tfidf_oh uh', 'tfidf_oh well', 'tfidf_oh yeah', 'tfidf_oh yes', 'tfidf_oh you', 'tfidf_oil', 'tfidf_okay', 'tfidf_old', 'tfidf_older', 'tfidf_on', 'tfidf_on it', 'tfidf_on my', 'tfidf_on on', 'tfidf_on that', 'tfidf_on the', 'tfidf_on uh', 'tfidf_on your', 'tfidf_once', 'tfidf_one', 'tfidf_one of', 'tfidf_one that', 'tfidf_one thing', 'tfidf_one time', 'tfidf_ones', 'tfidf_only', 'tfidf_only thing', 'tfidf_open', 'tfidf_or', 'tfidf_or anything', 'tfidf_or not', 'tfidf_or or', 'tfidf_or something', 'tfidf_or the', 'tfidf_or three', 'tfidf_or two', 'tfidf_or uh', 'tfidf_or whatever', 'tfidf_or you', 'tfidf_order', 'tfidf_other', 'tfidf_other people', 'tfidf_other than', 'tfidf_other thing', 'tfidf_other things', 'tfidf_ought', 'tfidf_ought to', 'tfidf_our', 'tfidf_our our', 'tfidf_ours', 'tfidf_out', 'tfidf_out and', 'tfidf_out in', 'tfidf_out of', 'tfidf_out on', 'tfidf_out that', 'tfidf_out there', 'tfidf_out to', 'tfidf_outside', 'tfidf_over', 'tfidf_over the', 'tfidf_over there', 'tfidf_own', 'tfidf_paid', 'tfidf_paper', 'tfidf_parents', 'tfidf_part', 'tfidf_part of', 'tfidf_particular', 'tfidf_particularly', 'tfidf_parts', 'tfidf_party', 'tfidf_past', 'tfidf_pause', 'tfidf_pause uh', 'tfidf_pause you', 'tfidf_pay', 'tfidf_pay for', 'tfidf_paying', 'tfidf_payment', 'tfidf_people', 'tfidf_people are', 'tfidf_people have', 'tfidf_people in', 'tfidf_people that', 'tfidf_people who', 'tfidf_per', 'tfidf_percent', 'tfidf_perhaps', 'tfidf_period', 'tfidf_person', 'tfidf_personal', 'tfidf_phone', 'tfidf_pick', 'tfidf_place', 'tfidf_places', 'tfidf_plan', 'tfidf_plano', 'tfidf_plants', 'tfidf_play', 'tfidf_played', 'tfidf_playing', 'tfidf_point', 'tfidf_policy', 'tfidf_pollution', 'tfidf_president', 'tfidf_pretty', 'tfidf_pretty good', 'tfidf_pretty much', 'tfidf_probably', 'tfidf_probably the', 'tfidf_problem', 'tfidf_problem is', 'tfidf_problem with', 'tfidf_problems', 'tfidf_process', 'tfidf_program', 'tfidf_programs', 'tfidf_public', 'tfidf_pull', 'tfidf_punishment', 'tfidf_put', 'tfidf_put it', 'tfidf_putting', 'tfidf_quality', 'tfidf_question', 'tfidf_quite', 'tfidf_quite bit', 'tfidf_radio', 'tfidf_rangers', 'tfidf_rather', 'tfidf_rather than', 'tfidf_re', 'tfidf_re all', 'tfidf_re doing', 'tfidf_re going', 'tfidf_re in', 'tfidf_re just', 'tfidf_re kind', 'tfidf_re not', 'tfidf_re really', 'tfidf_re right', 'tfidf_re talking', 'tfidf_re they', 'tfidf_re trying', 'tfidf_re uh', 'tfidf_re we', 'tfidf_read', 'tfidf_read the', 'tfidf_reading', 'tfidf_ready', 'tfidf_real', 'tfidf_real good', 'tfidf_realize', 'tfidf_really', 'tfidf_really do', 'tfidf_really don', 'tfidf_really good', 'tfidf_really have', 'tfidf_really is', 'tfidf_really like', 'tfidf_really nice', 'tfidf_really think', 'tfidf_reason', 'tfidf_recently', 'tfidf_recycling', 'tfidf_regular', 'tfidf_remember', 'tfidf_rent', 'tfidf_research', 'tfidf_rest', 'tfidf_rid', 'tfidf_rid of', 'tfidf_right', 'tfidf_right now', 'tfidf_road', 'tfidf_room', 'tfidf_run', 'tfidf_running', 'tfidf_sad', 'tfidf_said', 'tfidf_said that', 'tfidf_said you', 'tfidf_same', 'tfidf_same thing', 'tfidf_saw', 'tfidf_say', 'tfidf_say that', 'tfidf_say you', 'tfidf_saying', 'tfidf_says', 'tfidf_scary', 'tfidf_school', 'tfidf_schools', 'tfidf_science', 'tfidf_season', 'tfidf_second', 'tfidf_see', 'tfidf_see it', 'tfidf_see that', 'tfidf_see the', 'tfidf_seem', 'tfidf_seem to', 'tfidf_seems', 'tfidf_seems like', 'tfidf_seems to', 'tfidf_seen', 'tfidf_seen that', 'tfidf_self', 'tfidf_sell', 'tfidf_send', 'tfidf_sense', 'tfidf_serious', 'tfidf_service', 'tfidf_set', 'tfidf_seven', 'tfidf_seventy', 'tfidf_several', 'tfidf_she', 'tfidf_she had', 'tfidf_she has', 'tfidf_she just', 'tfidf_she was', 'tfidf_short', 'tfidf_should', 'tfidf_should be', 'tfidf_should have', 'tfidf_show', 'tfidf_shows', 'tfidf_side', 'tfidf_side of', 'tfidf_sil', 'tfidf_since', 'tfidf_single', 'tfidf_sister', 'tfidf_sit', 'tfidf_sitting', 'tfidf_situation', 'tfidf_six', 'tfidf_sixty', 'tfidf_size', 'tfidf_small', 'tfidf_so', 'tfidf_so don', 'tfidf_so he', 'tfidf_so if', 'tfidf_so it', 'tfidf_so many', 'tfidf_so much', 'tfidf_so that', 'tfidf_so the', 'tfidf_so there', 'tfidf_so they', 'tfidf_so think', 'tfidf_so uh', 'tfidf_so we', 'tfidf_so you', 'tfidf_social', 'tfidf_society', 'tfidf_some', 'tfidf_some kind', 'tfidf_some of', 'tfidf_some people', 'tfidf_some uh', 'tfidf_somebody', 'tfidf_someone', 'tfidf_something', 'tfidf_something like', 'tfidf_something that', 'tfidf_something to', 'tfidf_sometimes', 'tfidf_somewhere', 'tfidf_son', 'tfidf_sort', 'tfidf_sort of', 'tfidf_sounds', 'tfidf_sounds like', 'tfidf_south', 'tfidf_space', 'tfidf_spend', 'tfidf_spending', 'tfidf_spent', 'tfidf_sports', 'tfidf_spring', 'tfidf_start', 'tfidf_started', 'tfidf_starting', 'tfidf_starts', 'tfidf_state', 'tfidf_states', 'tfidf_stay', 'tfidf_stayed', 'tfidf_still', 'tfidf_stop', 'tfidf_story', 'tfidf_strange', 'tfidf_street', 'tfidf_student', 'tfidf_students', 'tfidf_stuff', 'tfidf_stuff like', 'tfidf_such', 'tfidf_summer', 'tfidf_support', 'tfidf_supposed', 'tfidf_supposed to', 'tfidf_sure', 'tfidf_sure that', 'tfidf_system', 'tfidf_systems', 'tfidf_take', 'tfidf_take care', 'tfidf_take it', 'tfidf_taken', 'tfidf_takes', 'tfidf_taking', 'tfidf_talk', 'tfidf_talk about', 'tfidf_talk to', 'tfidf_talked', 'tfidf_talked to', 'tfidf_talking', 'tfidf_talking about', 'tfidf_talking to', 'tfidf_tax', 'tfidf_taxes', 'tfidf_teach', 'tfidf_teacher', 'tfidf_teachers', 'tfidf_team', 'tfidf_television', 'tfidf_tell', 'tfidf_tell you', 'tfidf_telling', 'tfidf_ten', 'tfidf_ten years', 'tfidf_tend', 'tfidf_tend to', 'tfidf_term', 'tfidf_terms', 'tfidf_terms of', 'tfidf_test', 'tfidf_testing', 'tfidf_texas', 'tfidf_th', 'tfidf_tha', 'tfidf_than', 'tfidf_than that', 'tfidf_that', 'tfidf_that about', 'tfidf_that all', 'tfidf_that and', 'tfidf_that are', 'tfidf_that because', 'tfidf_that can', 'tfidf_that could', 'tfidf_that don', 'tfidf_that for', 'tfidf_that going', 'tfidf_that good', 'tfidf_that great', 'tfidf_that had', 'tfidf_that has', 'tfidf_that have', 'tfidf_that he', 'tfidf_that if', 'tfidf_that in', 'tfidf_that interesting', 'tfidf_that is', 'tfidf_that it', 'tfidf_that just', 'tfidf_that kind', 'tfidf_that like', 'tfidf_that lot', 'tfidf_that makes', 'tfidf_that might', 'tfidf_that much', 'tfidf_that my', 'tfidf_that not', 'tfidf_that on', 'tfidf_that one', 'tfidf_that pretty', 'tfidf_that probably', 'tfidf_that really', 'tfidf_that right', 'tfidf_that she', 'tfidf_that sort', 'tfidf_that sounds', 'tfidf_that that', 'tfidf_that the', 'tfidf_that there', 'tfidf_that they', 'tfidf_that think', 'tfidf_that time', 'tfidf_that too', 'tfidf_that true', 'tfidf_that uh', 'tfidf_that um', 'tfidf_that ve', 'tfidf_that very', 'tfidf_that was', 'tfidf_that way', 'tfidf_that we', 'tfidf_that were', 'tfidf_that what', 'tfidf_that when', 'tfidf_that where', 'tfidf_that why', 'tfidf_that will', 'tfidf_that would', 'tfidf_that you', 'tfidf_the', 'tfidf_the air', 'tfidf_the best', 'tfidf_the big', 'tfidf_the biggest', 'tfidf_the car', 'tfidf_the city', 'tfidf_the company', 'tfidf_the country', 'tfidf_the day', 'tfidf_the end', 'tfidf_the fact', 'tfidf_the family', 'tfidf_the first', 'tfidf_the government', 'tfidf_the guy', 'tfidf_the home', 'tfidf_the house', 'tfidf_the idea', 'tfidf_the in', 'tfidf_the kids', 'tfidf_the last', 'tfidf_the middle', 'tfidf_the money', 'tfidf_the more', 'tfidf_the most', 'tfidf_the movie', 'tfidf_the news', 'tfidf_the next', 'tfidf_the old', 'tfidf_the one', 'tfidf_the ones', 'tfidf_the only', 'tfidf_the other', 'tfidf_the paper', 'tfidf_the past', 'tfidf_the people', 'tfidf_the point', 'tfidf_the problem', 'tfidf_the right', 'tfidf_the same', 'tfidf_the school', 'tfidf_the state', 'tfidf_the the', 'tfidf_the thing', 'tfidf_the things', 'tfidf_the time', 'tfidf_the top', 'tfidf_the uh', 'tfidf_the united', 'tfidf_the way', 'tfidf_the whole', 'tfidf_the work', 'tfidf_the world', 'tfidf_the you', 'tfidf_their', 'tfidf_their own', 'tfidf_them', 'tfidf_them and', 'tfidf_them are', 'tfidf_them in', 'tfidf_them out', 'tfidf_them to', 'tfidf_them you', 'tfidf_themselves', 'tfidf_then', 'tfidf_then it', 'tfidf_then the', 'tfidf_then they', 'tfidf_then uh', 'tfidf_then we', 'tfidf_then when', 'tfidf_then you', 'tfidf_there', 'tfidf_there and', 'tfidf_there are', 'tfidf_there for', 'tfidf_there in', 'tfidf_there is', 'tfidf_there lot', 'tfidf_there no', 'tfidf_there not', 'tfidf_there some', 'tfidf_there that', 'tfidf_there there', 'tfidf_there was', 'tfidf_there were', 'tfidf_there you', 'tfidf_these', 'tfidf_these days', 'tfidf_these people', 'tfidf_they', 'tfidf_they are', 'tfidf_they can', 'tfidf_they come', 'tfidf_they could', 'tfidf_they did', 'tfidf_they didn', 'tfidf_they do', 'tfidf_they don', 'tfidf_they get', 'tfidf_they go', 'tfidf_they got', 'tfidf_they had', 'tfidf_they have', 'tfidf_they just', 'tfidf_they ll', 'tfidf_they make', 'tfidf_they need', 'tfidf_they put', 'tfidf_they re', 'tfidf_they really', 'tfidf_they said', 'tfidf_they say', 'tfidf_they should', 'tfidf_they they', 'tfidf_they think', 'tfidf_they uh', 'tfidf_they ve', 'tfidf_they want', 'tfidf_they were', 'tfidf_they would', 'tfidf_thing', 'tfidf_thing is', 'tfidf_thing that', 'tfidf_things', 'tfidf_things like', 'tfidf_things that', 'tfidf_think', 'tfidf_think about', 'tfidf_think if', 'tfidf_think in', 'tfidf_think it', 'tfidf_think of', 'tfidf_think that', 'tfidf_think the', 'tfidf_think there', 'tfidf_think they', 'tfidf_think think', 'tfidf_think uh', 'tfidf_think we', 'tfidf_think you', 'tfidf_thinking', 'tfidf_thirty', 'tfidf_this', 'tfidf_this is', 'tfidf_this was', 'tfidf_this year', 'tfidf_those', 'tfidf_those are', 'tfidf_those things', 'tfidf_though', 'tfidf_thought', 'tfidf_thought that', 'tfidf_thousand', 'tfidf_thousand dollars', 'tfidf_three', 'tfidf_through', 'tfidf_through the', 'tfidf_throw', 'tfidf_ti', 'tfidf_time', 'tfidf_time to', 'tfidf_times', 'tfidf_to', 'tfidf_to be', 'tfidf_to buy', 'tfidf_to come', 'tfidf_to do', 'tfidf_to find', 'tfidf_to get', 'tfidf_to give', 'tfidf_to go', 'tfidf_to happen', 'tfidf_to have', 'tfidf_to it', 'tfidf_to just', 'tfidf_to keep', 'tfidf_to know', 'tfidf_to learn', 'tfidf_to look', 'tfidf_to make', 'tfidf_to me', 'tfidf_to my', 'tfidf_to pay', 'tfidf_to play', 'tfidf_to put', 'tfidf_to read', 'tfidf_to say', 'tfidf_to school', 'tfidf_to see', 'tfidf_to spend', 'tfidf_to start', 'tfidf_to stay', 'tfidf_to take', 'tfidf_to talk', 'tfidf_to that', 'tfidf_to the', 'tfidf_to them', 'tfidf_to think', 'tfidf_to to', 'tfidf_to try', 'tfidf_to uh', 'tfidf_to use', 'tfidf_to watch', 'tfidf_to work', 'tfidf_to you', 'tfidf_today', 'tfidf_together', 'tfidf_told', 'tfidf_too', 'tfidf_too many', 'tfidf_too much', 'tfidf_took', 'tfidf_top', 'tfidf_topic', 'tfidf_totally', 'tfidf_tough', 'tfidf_town', 'tfidf_training', 'tfidf_trees', 'tfidf_tried', 'tfidf_tried to', 'tfidf_trouble', 'tfidf_true', 'tfidf_try', 'tfidf_try to', 'tfidf_trying', 'tfidf_trying to', 'tfidf_turn', 'tfidf_tv', 'tfidf_twelve', 'tfidf_twenty', 'tfidf_two', 'tfidf_two hundred', 'tfidf_two or', 'tfidf_two years', 'tfidf_type', 'tfidf_type of', 'tfidf_uh', 'tfidf_uh and', 'tfidf_uh as', 'tfidf_uh at', 'tfidf_uh but', 'tfidf_uh do', 'tfidf_uh don', 'tfidf_uh for', 'tfidf_uh guess', 'tfidf_uh have', 'tfidf_uh he', 'tfidf_uh huh', 'tfidf_uh if', 'tfidf_uh in', 'tfidf_uh is', 'tfidf_uh it', 'tfidf_uh just', 'tfidf_uh like', 'tfidf_uh my', 'tfidf_uh oh', 'tfidf_uh one', 'tfidf_uh pause', 'tfidf_uh really', 'tfidf_uh she', 'tfidf_uh so', 'tfidf_uh some', 'tfidf_uh that', 'tfidf_uh the', 'tfidf_uh there', 'tfidf_uh they', 'tfidf_uh think', 'tfidf_uh this', 'tfidf_uh to', 'tfidf_uh uh', 'tfidf_uh ve', 'tfidf_uh was', 'tfidf_uh we', 'tfidf_uh well', 'tfidf_uh what', 'tfidf_uh when', 'tfidf_uh yeah', 'tfidf_uh you', 'tfidf_um', 'tfidf_um it', 'tfidf_um that', 'tfidf_um the', 'tfidf_um they', 'tfidf_um uh', 'tfidf_um you', 'tfidf_under', 'tfidf_understand', 'tfidf_unfortunately', 'tfidf_united', 'tfidf_united states', 'tfidf_university', 'tfidf_unless', 'tfidf_until', 'tfidf_up', 'tfidf_up and', 'tfidf_up here', 'tfidf_up in', 'tfidf_up on', 'tfidf_up the', 'tfidf_up there', 'tfidf_up to', 'tfidf_up with', 'tfidf_us', 'tfidf_use', 'tfidf_use it', 'tfidf_used', 'tfidf_used to', 'tfidf_using', 'tfidf_usually', 'tfidf_vacation', 'tfidf_van', 'tfidf_ve', 'tfidf_ve always', 'tfidf_ve been', 'tfidf_ve done', 'tfidf_ve ever', 'tfidf_ve got', 'tfidf_ve had', 'tfidf_ve heard', 'tfidf_ve just', 'tfidf_ve never', 'tfidf_ve seen', 'tfidf_ve ve', 'tfidf_very', 'tfidf_very good', 'tfidf_very much', 'tfidf_very very', 'tfidf_video', 'tfidf_vote', 'tfidf_wait', 'tfidf_waiting', 'tfidf_walk', 'tfidf_want', 'tfidf_want to', 'tfidf_wanted', 'tfidf_wanted to', 'tfidf_wants', 'tfidf_war', 'tfidf_was', 'tfidf_was going', 'tfidf_was good', 'tfidf_was in', 'tfidf_was it', 'tfidf_was just', 'tfidf_was kind', 'tfidf_was like', 'tfidf_was nice', 'tfidf_was not', 'tfidf_was one', 'tfidf_was pretty', 'tfidf_was really', 'tfidf_was so', 'tfidf_was that', 'tfidf_was the', 'tfidf_was uh', 'tfidf_was very', 'tfidf_was you', 'tfidf_washington', 'tfidf_wasn', 'tfidf_watch', 'tfidf_watching', 'tfidf_water', 'tfidf_way', 'tfidf_way it', 'tfidf_way to', 'tfidf_ways', 'tfidf_we', 'tfidf_we all', 'tfidf_we are', 'tfidf_we can', 'tfidf_we could', 'tfidf_we did', 'tfidf_we didn', 'tfidf_we do', 'tfidf_we don', 'tfidf_we get', 'tfidf_we go', 'tfidf_we got', 'tfidf_we had', 'tfidf_we have', 'tfidf_we just', 'tfidf_we live', 'tfidf_we ll', 'tfidf_we need', 'tfidf_we re', 'tfidf_we really', 'tfidf_we should', 'tfidf_we uh', 'tfidf_we ve', 'tfidf_we we', 'tfidf_we went', 'tfidf_we were', 'tfidf_wear', 'tfidf_weather', 'tfidf_week', 'tfidf_weekend', 'tfidf_weeks', 'tfidf_well', 'tfidf_well do', 'tfidf_well don', 'tfidf_well guess', 'tfidf_well have', 'tfidf_well it', 'tfidf_well my', 'tfidf_well that', 'tfidf_well the', 'tfidf_well they', 'tfidf_well think', 'tfidf_well uh', 'tfidf_well ve', 'tfidf_well we', 'tfidf_well yeah', 'tfidf_well you', 'tfidf_went', 'tfidf_went to', 'tfidf_were', 'tfidf_were in', 'tfidf_weren', 'tfidf_west', 'tfidf_what', 'tfidf_what going', 'tfidf_what he', 'tfidf_what is', 'tfidf_what it', 'tfidf_what the', 'tfidf_what they', 'tfidf_what was', 'tfidf_what we', 'tfidf_what you', 'tfidf_whatever', 'tfidf_when', 'tfidf_when he', 'tfidf_when it', 'tfidf_when the', 'tfidf_when they', 'tfidf_when was', 'tfidf_when we', 'tfidf_when when', 'tfidf_when you', 'tfidf_where', 'tfidf_where it', 'tfidf_where the', 'tfidf_where they', 'tfidf_where we', 'tfidf_where you', 'tfidf_whether', 'tfidf_which', 'tfidf_which is', 'tfidf_while', 'tfidf_who', 'tfidf_who are', 'tfidf_whole', 'tfidf_whole lot', 'tfidf_why', 'tfidf_wife', 'tfidf_wife and', 'tfidf_will', 'tfidf_will be', 'tfidf_wind', 'tfidf_winter', 'tfidf_wish', 'tfidf_with', 'tfidf_with it', 'tfidf_with my', 'tfidf_with that', 'tfidf_with the', 'tfidf_with their', 'tfidf_with them', 'tfidf_with uh', 'tfidf_with you', 'tfidf_within', 'tfidf_without', 'tfidf_woman', 'tfidf_women', 'tfidf_won', 'tfidf_wonder', 'tfidf_wonderful', 'tfidf_work', 'tfidf_work for', 'tfidf_work in', 'tfidf_work with', 'tfidf_worked', 'tfidf_working', 'tfidf_works', 'tfidf_world', 'tfidf_worse', 'tfidf_worth', 'tfidf_would', 'tfidf_would be', 'tfidf_would have', 'tfidf_would like', 'tfidf_would say', 'tfidf_wouldn', 'tfidf_wouldn have', 'tfidf_wow', 'tfidf_wrong', 'tfidf_yard', 'tfidf_yeah', 'tfidf_year', 'tfidf_year old', 'tfidf_years', 'tfidf_years ago', 'tfidf_years old', 'tfidf_yep', 'tfidf_yes', 'tfidf_yet', 'tfidf_yo', 'tfidf_you', 'tfidf_you are', 'tfidf_you can', 'tfidf_you could', 'tfidf_you do', 'tfidf_you don', 'tfidf_you ever', 'tfidf_you get', 'tfidf_you go', 'tfidf_you got', 'tfidf_you had', 'tfidf_you have', 'tfidf_you just', 'tfidf_you know', 'tfidf_you like', 'tfidf_you look', 'tfidf_you need', 'tfidf_you put', 'tfidf_you re', 'tfidf_you really', 'tfidf_you said', 'tfidf_you say', 'tfidf_you see', 'tfidf_you take', 'tfidf_you think', 'tfidf_you to', 'tfidf_you too', 'tfidf_you uh', 'tfidf_you ve', 'tfidf_you want', 'tfidf_you were', 'tfidf_you work', 'tfidf_you would', 'tfidf_you you', 'tfidf_young', 'tfidf_your', 'tfidf_yourself', 'bert_0', 'bert_1', 'bert_2', 'bert_3', 'bert_4', 'bert_5', 'bert_6', 'bert_7', 'bert_8', 'bert_9', 'bert_10', 'bert_11', 'bert_12', 'bert_13', 'bert_14', 'bert_15', 'bert_16', 'bert_17', 'bert_18', 'bert_19', 'bert_20', 'bert_21', 'bert_22', 'bert_23', 'bert_24', 'bert_25', 'bert_26', 'bert_27', 'bert_28', 'bert_29', 'bert_30', 'bert_31', 'bert_32', 'bert_33', 'bert_34', 'bert_35', 'bert_36', 'bert_37', 'bert_38', 'bert_39', 'bert_40', 'bert_41', 'bert_42', 'bert_43', 'bert_44', 'bert_45', 'bert_46', 'bert_47', 'bert_48', 'bert_49', 'bert_50', 'bert_51', 'bert_52', 'bert_53', 'bert_54', 'bert_55', 'bert_56', 'bert_57', 'bert_58', 'bert_59', 'bert_60', 'bert_61', 'bert_62', 'bert_63', 'bert_64', 'bert_65', 'bert_66', 'bert_67', 'bert_68', 'bert_69', 'bert_70', 'bert_71', 'bert_72', 'bert_73', 'bert_74', 'bert_75', 'bert_76', 'bert_77', 'bert_78', 'bert_79', 'bert_80', 'bert_81', 'bert_82', 'bert_83', 'bert_84', 'bert_85', 'bert_86', 'bert_87', 'bert_88', 'bert_89', 'bert_90', 'bert_91', 'bert_92', 'bert_93', 'bert_94', 'bert_95', 'bert_96', 'bert_97', 'bert_98', 'bert_99', 'bert_100', 'bert_101', 'bert_102', 'bert_103', 'bert_104', 'bert_105', 'bert_106', 'bert_107', 'bert_108', 'bert_109', 'bert_110', 'bert_111', 'bert_112', 'bert_113', 'bert_114', 'bert_115', 'bert_116', 'bert_117', 'bert_118', 'bert_119', 'bert_120', 'bert_121', 'bert_122', 'bert_123', 'bert_124', 'bert_125', 'bert_126', 'bert_127', 'bert_128', 'bert_129', 'bert_130', 'bert_131', 'bert_132', 'bert_133', 'bert_134', 'bert_135', 'bert_136', 'bert_137', 'bert_138', 'bert_139', 'bert_140', 'bert_141', 'bert_142', 'bert_143', 'bert_144', 'bert_145', 'bert_146', 'bert_147', 'bert_148', 'bert_149', 'bert_150', 'bert_151', 'bert_152', 'bert_153', 'bert_154', 'bert_155', 'bert_156', 'bert_157', 'bert_158', 'bert_159', 'bert_160', 'bert_161', 'bert_162', 'bert_163', 'bert_164', 'bert_165', 'bert_166', 'bert_167', 'bert_168', 'bert_169', 'bert_170', 'bert_171', 'bert_172', 'bert_173', 'bert_174', 'bert_175', 'bert_176', 'bert_177', 'bert_178', 'bert_179', 'bert_180', 'bert_181', 'bert_182', 'bert_183', 'bert_184', 'bert_185', 'bert_186', 'bert_187', 'bert_188', 'bert_189', 'bert_190', 'bert_191', 'bert_192', 'bert_193', 'bert_194', 'bert_195', 'bert_196', 'bert_197', 'bert_198', 'bert_199', 'bert_200', 'bert_201', 'bert_202', 'bert_203', 'bert_204', 'bert_205', 'bert_206', 'bert_207', 'bert_208', 'bert_209', 'bert_210', 'bert_211', 'bert_212', 'bert_213', 'bert_214', 'bert_215', 'bert_216', 'bert_217', 'bert_218', 'bert_219', 'bert_220', 'bert_221', 'bert_222', 'bert_223', 'bert_224', 'bert_225', 'bert_226', 'bert_227', 'bert_228', 'bert_229', 'bert_230', 'bert_231', 'bert_232', 'bert_233', 'bert_234', 'bert_235', 'bert_236', 'bert_237', 'bert_238', 'bert_239', 'bert_240', 'bert_241', 'bert_242', 'bert_243', 'bert_244', 'bert_245', 'bert_246', 'bert_247', 'bert_248', 'bert_249', 'bert_250', 'bert_251', 'bert_252', 'bert_253', 'bert_254', 'bert_255', 'bert_256', 'bert_257', 'bert_258', 'bert_259', 'bert_260', 'bert_261', 'bert_262', 'bert_263', 'bert_264', 'bert_265', 'bert_266', 'bert_267', 'bert_268', 'bert_269', 'bert_270', 'bert_271', 'bert_272', 'bert_273', 'bert_274', 'bert_275', 'bert_276', 'bert_277', 'bert_278', 'bert_279', 'bert_280', 'bert_281', 'bert_282', 'bert_283', 'bert_284', 'bert_285', 'bert_286', 'bert_287', 'bert_288', 'bert_289', 'bert_290', 'bert_291', 'bert_292', 'bert_293', 'bert_294', 'bert_295', 'bert_296', 'bert_297', 'bert_298', 'bert_299', 'bert_300', 'bert_301', 'bert_302', 'bert_303', 'bert_304', 'bert_305', 'bert_306', 'bert_307', 'bert_308', 'bert_309', 'bert_310', 'bert_311', 'bert_312', 'bert_313', 'bert_314', 'bert_315', 'bert_316', 'bert_317', 'bert_318', 'bert_319', 'bert_320', 'bert_321', 'bert_322', 'bert_323', 'bert_324', 'bert_325', 'bert_326', 'bert_327', 'bert_328', 'bert_329', 'bert_330', 'bert_331', 'bert_332', 'bert_333', 'bert_334', 'bert_335', 'bert_336', 'bert_337', 'bert_338', 'bert_339', 'bert_340', 'bert_341', 'bert_342', 'bert_343', 'bert_344', 'bert_345', 'bert_346', 'bert_347', 'bert_348', 'bert_349', 'bert_350', 'bert_351', 'bert_352', 'bert_353', 'bert_354', 'bert_355', 'bert_356', 'bert_357', 'bert_358', 'bert_359', 'bert_360', 'bert_361', 'bert_362', 'bert_363', 'bert_364', 'bert_365', 'bert_366', 'bert_367', 'bert_368', 'bert_369', 'bert_370', 'bert_371', 'bert_372', 'bert_373', 'bert_374', 'bert_375', 'bert_376', 'bert_377', 'bert_378', 'bert_379', 'bert_380', 'bert_381', 'bert_382', 'bert_383', 'bert_384', 'bert_385', 'bert_386', 'bert_387', 'bert_388', 'bert_389', 'bert_390', 'bert_391', 'bert_392', 'bert_393', 'bert_394', 'bert_395', 'bert_396', 'bert_397', 'bert_398', 'bert_399', 'bert_400', 'bert_401', 'bert_402', 'bert_403', 'bert_404', 'bert_405', 'bert_406', 'bert_407', 'bert_408', 'bert_409', 'bert_410', 'bert_411', 'bert_412', 'bert_413', 'bert_414', 'bert_415', 'bert_416', 'bert_417', 'bert_418', 'bert_419', 'bert_420', 'bert_421', 'bert_422', 'bert_423', 'bert_424', 'bert_425', 'bert_426', 'bert_427', 'bert_428', 'bert_429', 'bert_430', 'bert_431', 'bert_432', 'bert_433', 'bert_434', 'bert_435', 'bert_436', 'bert_437', 'bert_438', 'bert_439', 'bert_440', 'bert_441', 'bert_442', 'bert_443', 'bert_444', 'bert_445', 'bert_446', 'bert_447', 'bert_448', 'bert_449', 'bert_450', 'bert_451', 'bert_452', 'bert_453', 'bert_454', 'bert_455', 'bert_456', 'bert_457', 'bert_458', 'bert_459', 'bert_460', 'bert_461', 'bert_462', 'bert_463', 'bert_464', 'bert_465', 'bert_466', 'bert_467', 'bert_468', 'bert_469', 'bert_470', 'bert_471', 'bert_472', 'bert_473', 'bert_474', 'bert_475', 'bert_476', 'bert_477', 'bert_478', 'bert_479', 'bert_480', 'bert_481', 'bert_482', 'bert_483', 'bert_484', 'bert_485', 'bert_486', 'bert_487', 'bert_488', 'bert_489', 'bert_490', 'bert_491', 'bert_492', 'bert_493', 'bert_494', 'bert_495', 'bert_496', 'bert_497', 'bert_498', 'bert_499', 'bert_500', 'bert_501', 'bert_502', 'bert_503', 'bert_504', 'bert_505', 'bert_506', 'bert_507', 'bert_508', 'bert_509', 'bert_510', 'bert_511', 'bert_512', 'bert_513', 'bert_514', 'bert_515', 'bert_516', 'bert_517', 'bert_518', 'bert_519', 'bert_520', 'bert_521', 'bert_522', 'bert_523', 'bert_524', 'bert_525', 'bert_526', 'bert_527', 'bert_528', 'bert_529', 'bert_530', 'bert_531', 'bert_532', 'bert_533', 'bert_534', 'bert_535', 'bert_536', 'bert_537', 'bert_538', 'bert_539', 'bert_540', 'bert_541', 'bert_542', 'bert_543', 'bert_544', 'bert_545', 'bert_546', 'bert_547', 'bert_548', 'bert_549', 'bert_550', 'bert_551', 'bert_552', 'bert_553', 'bert_554', 'bert_555', 'bert_556', 'bert_557', 'bert_558', 'bert_559', 'bert_560', 'bert_561', 'bert_562', 'bert_563', 'bert_564', 'bert_565', 'bert_566', 'bert_567', 'bert_568', 'bert_569', 'bert_570', 'bert_571', 'bert_572', 'bert_573', 'bert_574', 'bert_575', 'bert_576', 'bert_577', 'bert_578', 'bert_579', 'bert_580', 'bert_581', 'bert_582', 'bert_583', 'bert_584', 'bert_585', 'bert_586', 'bert_587', 'bert_588', 'bert_589', 'bert_590', 'bert_591', 'bert_592', 'bert_593', 'bert_594', 'bert_595', 'bert_596', 'bert_597', 'bert_598', 'bert_599', 'bert_600', 'bert_601', 'bert_602', 'bert_603', 'bert_604', 'bert_605', 'bert_606', 'bert_607', 'bert_608', 'bert_609', 'bert_610', 'bert_611', 'bert_612', 'bert_613', 'bert_614', 'bert_615', 'bert_616', 'bert_617', 'bert_618', 'bert_619', 'bert_620', 'bert_621', 'bert_622', 'bert_623', 'bert_624', 'bert_625', 'bert_626', 'bert_627', 'bert_628', 'bert_629', 'bert_630', 'bert_631', 'bert_632', 'bert_633', 'bert_634', 'bert_635', 'bert_636', 'bert_637', 'bert_638', 'bert_639', 'bert_640', 'bert_641', 'bert_642', 'bert_643', 'bert_644', 'bert_645', 'bert_646', 'bert_647', 'bert_648', 'bert_649', 'bert_650', 'bert_651', 'bert_652', 'bert_653', 'bert_654', 'bert_655', 'bert_656', 'bert_657', 'bert_658', 'bert_659', 'bert_660', 'bert_661', 'bert_662', 'bert_663', 'bert_664', 'bert_665', 'bert_666', 'bert_667', 'bert_668', 'bert_669', 'bert_670', 'bert_671', 'bert_672', 'bert_673', 'bert_674', 'bert_675', 'bert_676', 'bert_677', 'bert_678', 'bert_679', 'bert_680', 'bert_681', 'bert_682', 'bert_683', 'bert_684', 'bert_685', 'bert_686', 'bert_687', 'bert_688', 'bert_689', 'bert_690', 'bert_691', 'bert_692', 'bert_693', 'bert_694', 'bert_695', 'bert_696', 'bert_697', 'bert_698', 'bert_699', 'bert_700', 'bert_701', 'bert_702', 'bert_703', 'bert_704', 'bert_705', 'bert_706', 'bert_707', 'bert_708', 'bert_709', 'bert_710', 'bert_711', 'bert_712', 'bert_713', 'bert_714', 'bert_715', 'bert_716', 'bert_717', 'bert_718', 'bert_719', 'bert_720', 'bert_721', 'bert_722', 'bert_723', 'bert_724', 'bert_725', 'bert_726', 'bert_727', 'bert_728', 'bert_729', 'bert_730', 'bert_731', 'bert_732', 'bert_733', 'bert_734', 'bert_735', 'bert_736', 'bert_737', 'bert_738', 'bert_739', 'bert_740', 'bert_741', 'bert_742', 'bert_743', 'bert_744', 'bert_745', 'bert_746', 'bert_747', 'bert_748', 'bert_749', 'bert_750', 'bert_751', 'bert_752', 'bert_753', 'bert_754', 'bert_755', 'bert_756', 'bert_757', 'bert_758', 'bert_759', 'bert_760', 'bert_761', 'bert_762', 'bert_763', 'bert_764', 'bert_765', 'bert_766', 'bert_767']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "text_train = pd.read_csv(os.path.join(OUT_DIR, \"text_features_train.csv\"))\n",
    "text_valid = pd.read_csv(os.path.join(OUT_DIR, \"text_features_valid.csv\"))\n",
    "text_test  = pd.read_csv(os.path.join(OUT_DIR, \"text_features_test.csv\"))\n",
    "\n",
    "print(text_train.shape, text_valid.shape, text_test.shape)\n",
    "\n",
    "print(\"Text feature columns:\", len(text_train.columns))\n",
    "print(text_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c82d9",
   "metadata": {},
   "source": [
    "For the text-based modality, I designed a hybrid feature set that combines:\n",
    "\n",
    "1.\tStructural features\n",
    "2.\tPart-of-speech (POS) distribution features\n",
    "3.\tLIWC-style psycholinguistic features (provided with the dataset)\n",
    "4.\tTF-IDF lexical features (unigram + bigram)\n",
    "5.  Contextual Embeddings from DistilBERT\n",
    "\n",
    "This combined representation aims to capture linguistic style and lexical content, which are both strongly associated with dialogue act categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d73e30",
   "metadata": {},
   "source": [
    "#### (A) Structural Features\n",
    "\n",
    "I extracted several interpretable surface-level structural features from each transcript, including:\n",
    "*\tCharacter length (len_chars)\n",
    "*\tToken length (len_tokens)\n",
    "*\tPunctuation count (num_punct)\n",
    "*\tWhether the utterance ends with a question mark (ends_question)\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "These features capture global structural properties of the utterance.\n",
    "Many dialogue acts, such as Questions, tend to have short lengths and end with “?”\n",
    "Acts like Statement-opinion or Statement-nonopinion are often longer and contain more complex punctuation\n",
    "Backchannels (e.g., “uh-huh”, “yeah”) are very short and have very low token counts\n",
    "Therefore, structural features are directly diagnostic of dialogue-act categories.\n",
    "\n",
    "**Extraction Technique：**\n",
    "\n",
    "All structural features were computed using simple tokenization via nltk.word_tokenize followed by direct string manipulations.\n",
    "This ensures fast and transparent feature extraction with no risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3afdf2e",
   "metadata": {},
   "source": [
    "#### (B) POS Distribution Features\n",
    "\n",
    "For each utterance, I computed the proportion of major POS categories:\n",
    "*\tpos_prop_NOUN\n",
    "*\tpos_prop_VERB\n",
    "*\tpos_prop_ADJ\n",
    "*\tpos_prop_ADV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3667739",
   "metadata": {},
   "source": [
    "**Reason:**\n",
    "\n",
    "The distribution of parts of speech reflects the functional role of the utterance:\n",
    "Questions typically have more verbs and adverbs (“how”, “what”, “do you”). Opinions tend to include many adjectives and adverbs (“really”, “pretty”, “good”, “bad”). Statements often contain more nouns, reflecting topic introduction. Backchannels and acknowledgements contain few POS items overall\n",
    "By converting counts into proportions, POS features become independent of utterance length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c85365",
   "metadata": {},
   "source": [
    "**Extraction Technique:**\n",
    "\n",
    "POS tagging was implemented using:\n",
    "```python\n",
    "from nltk import word_tokenize, pos_tag\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f32fd",
   "metadata": {},
   "source": [
    "#### (C) LIWC-style Psycholinguistic Features\n",
    "\n",
    "The dataset already includes a set of LIWC-derived counts for each utterance:\n",
    "\n",
    "Examples include: pronoun, auxverb, negemo, insight, tentat, differ, work, money, social, family, posemo, anger, sad, etc.\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "LIWC features capture psychological intent, social orientation, and cognitive state—all of which are strongly related to dialog acts.\n",
    "\n",
    "For examples, Utterances containing “I think”, “I guess”, “I believe” often indicate Statement-opinion. High negation or negative emotion may correspond to Disagreement. Frequent pronouns may correlate with Personal topics or Opinions. High insight or cogproc loadings indicate reasoning or explanation acts. These features help capture interpretive, semantic cues beyond raw text.\n",
    "\n",
    "**Extraction Technique:**\n",
    "\n",
    "These features were directly read from the CSV and merged with structural + TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fb1ab",
   "metadata": {},
   "source": [
    "#### (D) TF-IDF lexical features (Unigram + Bigram)\n",
    "\n",
    "I trained a TF-IDF vectorizer on the training transcripts:\n",
    "* n-gram range: (1, 2)\n",
    "*\tmax features: 2000\n",
    "*\tlowercasing + unicode normalization enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ddf0d5",
   "metadata": {},
   "source": [
    "**Reason:**\n",
    "\n",
    "TF-IDF features provide direct lexical cues.\n",
    "Unlike LIWC, TF-IDF captures specific content, such as “do you”, “are you” indicate very strong indicators of questions. “I think”, “I mean” means strong indicators of opinions. Topic-specific n-grams are useful for statements. Filler words (“uh”, “um”) are often correspond to backchannels or disfluencies\n",
    "TF-IDF provides a high-dimensional sparse representation that significantly boosts predictive power, especially for linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe29985",
   "metadata": {},
   "source": [
    "**Extraction Technique:**\n",
    "\n",
    "```python\n",
    "vectorizer = TfidfVectorizer(...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24c445",
   "metadata": {},
   "source": [
    "#### (E) Contextual Embeddings from DistilBERT\n",
    "Description:\n",
    "To complement traditional lexical and psycholinguistic features, we additionally extracted 768-dimensional contextual embeddings from DistilBERT (Sanh et al., 2019), a distilled version of BERT that retains 97% of its language understanding capacity while being 40% smaller and faster. For each utterance, we fed the text into the pretrained DistilBERT model and performed masked mean pooling over token hidden states to obtain a single sentence-level embedding.\n",
    "\n",
    "Reason:\n",
    "Unlike TF-IDF, which is purely frequency-based, and LIWC, which captures interpretable psychological categories, BERT-style embeddings encode contextualized semantics — words are represented differently depending on their surrounding words. This allows the model to distinguish subtle intent differences such as “I think” vs. “Do you think”, or “Yeah, right” (sarcastic agreement) vs. “Yeah, that’s correct” (affirmative).\n",
    "By combining these contextual vectors with LIWC and TF-IDF features, we can jointly model surface form, psychological signals, and deep semantic meaning, leading to a more comprehensive representation of dialog acts.\n",
    "\n",
    "Extraction Technique:\n",
    "We used the pretrained model \"distilbert-base-uncased\" from Hugging Face Transformers. Each utterance was tokenized (max len = 96), passed through the encoder on MPS, and pooled to a fixed-length vector. These embeddings were then concatenated with the structural, POS, LIWC, and TF-IDF features to form the final feature matrix before model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51356bf",
   "metadata": {},
   "source": [
    "#### Total text feature dimension: 2854\n",
    "\n",
    "This hybrid approach combines interpretable features with powerful lexical statistics, which together provide a rich linguistic representation well-suited for dialogue act classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b78606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74111, 29) (19156, 29) (23540, 29)\n",
      "Speech feature columns: 29\n",
      "['dialog_id', 'speaker', 'da_tag', 'start_time', 'end_time', 'pitch_min', 'pitch_max', 'pitch_mean', 'pitch_sd', 'int_min', 'int_max', 'int_mean', 'int_sd', 'jitter', 'shimmer', 'hnr_mean', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13']\n"
     ]
    }
   ],
   "source": [
    "speech_train = pd.read_csv(os.path.join(OUT_DIR, \"speech_features_train.csv\"))\n",
    "speech_valid = pd.read_csv(os.path.join(OUT_DIR, \"speech_features_valid.csv\"))\n",
    "speech_test  = pd.read_csv(os.path.join(OUT_DIR, \"speech_features_test.csv\"))\n",
    "\n",
    "print(speech_train.shape, speech_valid.shape, speech_test.shape)\n",
    "\n",
    "print(\"Speech feature columns:\", len(speech_train.columns))\n",
    "print(speech_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2c037",
   "metadata": {},
   "source": [
    "**Speech-based Feature Set Description**\n",
    "\n",
    "For the speech modality, I extracted a compact but informative set of 29 acoustic-prosodic features, designed to capture properties that are strongly linked to conversational structure and therefore highly relevant to dialogue act recognition (DAR). These features include pitch, intensity, jitter, shimmer, harmonicity (HNR), and MFCCs computed on each time-aligned audio segment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46f63c",
   "metadata": {},
   "source": [
    "#### (A) Pitch-related Features (F0 statistics)\n",
    "\n",
    "Features: pitch_min, pitch_max, pitch_mean, pitch_sd\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "Pitch (fundamental frequency, F0) is one of the most important signals in conversational pragmatics:\n",
    "* Yes/no questions tend to have higher final pitch or rising contours\n",
    "* Wh-questions often begin with a high pitch\n",
    "*\tBackchannels (e.g., “uh-huh”) tend to have low pitch variance\n",
    "*\tStatements typically exhibit mid-range, stable pitch patterns\n",
    "*\tEmphasis or opinions may show higher pitch range and variability\n",
    "\n",
    "By including min/max/mean/std, we capture both absolute pitch level and prosodic dynamics, which help differentiate question-like, expressive, or monotonic acts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe6f50",
   "metadata": {},
   "source": [
    "Technique Used: Pitch features were extracted using Praat autocorrelation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0676c",
   "metadata": {},
   "source": [
    "#### (B) Intensity-related Features\n",
    "\n",
    "Features: int_min, int_max, int_mean, int_sd\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "Intensity (loudness) reflects emphasis, engagement, and speech energy, which correlate with several dialog acts:\n",
    "* Greetings, Acknowledgements, and Backchannels are generally short and low-intensity\n",
    "*\tOpinions and Complaints tend to show higher average intensity\n",
    "*\tQuestions often show increased amplitude near phrase boundaries\n",
    "*\tCommands or Corrections may be louder or more forceful\n",
    "\n",
    "The combination of mean, max, and variability provides a robust representation of speaking style and emotional force.\n",
    "\n",
    "Technique Used: Using Praat’s intensity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276d3c2",
   "metadata": {},
   "source": [
    "#### (C) Voice Perturbation Features (Jitter & Shimmer)\n",
    "\n",
    "Features: jitter (local), shimmer (local)\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "Jitter and shimmer measure micro-variations in voice frequency and amplitude, respectively.\n",
    "\n",
    "These features are classically used in speech pathology and emotion recognition, but they are also informative for conversational behaviors:\n",
    "* High jitter/shimmer often occurs in hesitations, uncertainty, and self-corrections\n",
    "*\tBackchannels like “uh”, “um” have unstable voicing indicates higher jitter\n",
    "*\tConfident statements usually have stable voicing indicates lower jitter/shimmer\n",
    "Thus, jitter and shimmer provide cues for confidence, hesitation, and speaker affect, all relevant for DAR.\n",
    "\n",
    "Technique Used: Praat’s measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea65ef",
   "metadata": {},
   "source": [
    "#### (D) Harmonic-to-Noise Ratio (HNR)\n",
    "\n",
    "Feature: hnr_mean\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "HNR measures voice clarity and breathiness:\n",
    "* Low HNR may indicate breathy, tense, or uncertain speech, which is common in questions, disfluencies, corrections\n",
    "*\tHigh HNR corresponds to clear, stable phonation, which is typical of statements or confident opinions\n",
    "\n",
    "Thus, HNR provides a compact measure of vocal quality differences across dialogue acts.\n",
    "\n",
    "Technique Used: Praat’s measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f8420",
   "metadata": {},
   "source": [
    "#### (E) MFCCs (Mel-Frequency Cepstral Coefficients, 13 dimensions)\n",
    "\n",
    "Toolkit: Librosa (librosa.feature.mfcc)\n",
    "\n",
    "For every segment, I computed MFCC from 1 to 13 and took the mean value of each.\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "MFCCs capture the short-term spectral envelope — the “timbre” of the speech — which reflects vowel quality, articulation clarity, phoneme distribution and speaking style.\n",
    "\n",
    "These cues differ systematically across DA types. For example:\n",
    "* Backchannels / filled pauses (“uh-huh”, “mm”) show more low-frequency energy\n",
    "*\tQuestions show more high-frequency shift due to rising intonation\n",
    "*\tStatements show flatter MFCC contours\n",
    "\n",
    "MFCCs are widely used in speech recognition and emotion recognition, making them a robust addition to DAR.\n",
    "\n",
    "Technique Used: Librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9064530",
   "metadata": {},
   "source": [
    "## 2. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7cad77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['%', 'aa', 'b', 'ba', 'fc', 'ny', 'qy', 'sd', 'sv', 'x']\n",
      "da_tag\n",
      "x     27024\n",
      "sd    19149\n",
      "b      9925\n",
      "sv     7051\n",
      "%      4101\n",
      "aa     3136\n",
      "qy     1253\n",
      "ba     1063\n",
      "ny      771\n",
      "fc      638\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sorted(train[\"da_tag\"].unique()))\n",
    "print(train[\"da_tag\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703016ff",
   "metadata": {},
   "source": [
    "### Text-based Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aae082",
   "metadata": {},
   "source": [
    "**We hypothesized that Agree/Accept (aa) would show higher LIWC assent than other dialogue acts, because aa utterances typically contain “yes/yeah/right/okay”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4633f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class mean(assent):\n",
      "da_tag\n",
      "x     0.000356\n",
      "sd    0.001568\n",
      "b     0.773436\n",
      "sv    0.002087\n",
      "%     0.067691\n",
      "aa    0.455504\n",
      "qy    0.004391\n",
      "ba    0.016996\n",
      "ny    0.920603\n",
      "fc    0.171708\n",
      "Name: assent, dtype: float64\n",
      "\n",
      "Mann-Whitney U (aa > others): U=151738678.5, p=0.000e+00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"task1/text_features_train.csv\")\n",
    "\n",
    "order = df[\"da_tag\"].value_counts().index.tolist()\n",
    "\n",
    "assent_means = df.groupby(\"da_tag\")[\"assent\"].mean().reindex(order)\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=assent_means.index, y=assent_means.values)\n",
    "plt.title(\"Average LIWC 'assent' across Dialogue Acts\")\n",
    "plt.ylabel(\"mean(assent)\")\n",
    "plt.xlabel(\"da_tag\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Per-class mean(assent):\")\n",
    "print(assent_means)\n",
    "\n",
    "aa_vals = df.loc[df[\"da_tag\"]==\"aa\", \"assent\"].values\n",
    "other_vals = df.loc[df[\"da_tag\"]!=\"aa\", \"assent\"].values\n",
    "\n",
    "u_stat, p_u = stats.mannwhitneyu(aa_vals, other_vals, alternative=\"greater\")\n",
    "print(f\"\\nMann-Whitney U (aa > others): U={u_stat:.1f}, p={p_u:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80b8c3",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "Contrary to the hypothesis, when computing per-class means, the actual order was:\n",
    "*\tny (0.921) — highest\n",
    "*\tb (0.773)\n",
    "*\taa (0.456)\n",
    "*\tfc (0.172)\n",
    "*\t(others ≪ 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc30d0",
   "metadata": {},
   "source": [
    "A Mann–Whitney U test comparing aa vs all other classes yielded: U = 1.517e8, p ≈ 0\n",
    "\n",
    "Although the p-value indicates strong differences between groups, the direction contradicts the original hypothesis.\n",
    "aa does not exhibit the highest “assent” levels; instead, ny and b are higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7165c",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "1. Short-utterance length inflation\n",
    "    Many ny examples are extremely short responses (e.g., “yeah, no.”). In very short utterances, a single yeah dramatically increases the proportion of “assent” tokens, often near 1.0.\n",
    "\n",
    "2. “Yeah, no” patterns\n",
    "    ny (dispreferred answers) frequently begins with yeah but then negates the proposition (e.g., “yeah I don’t think so”). This causes assent to co-occur with negate, raising its average for ny.\n",
    "\n",
    "3. Backchannels (b) naturally contain assent tokens\n",
    "    Backchannel responses (e.g., “yeah”, “right”, “uh-huh”) often contain exactly the words captured by LIWC “assent,” making high values expected.\n",
    "\n",
    "4. Proportional LIWC features amplify certain DA types\n",
    "    Because “assent” is measured as proportion of tokens, shorter utterances inflate the value much more than longer agreement statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60fe610",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "The data does not support the hypothesis that assent is a distinctive marker of the aa dialogue act.\n",
    "Instead, the highest assent proportions occur in:ny (due to “yeah, no” constructions), and b (backchannels).\n",
    "\n",
    "Therefore, LIWC “assent” is not a clean indicator of Accept/Agree (aa), but rather reflects conversational micro-responses and dispreferred answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ff10d",
   "metadata": {},
   "source": [
    "### Speech-based Feature Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083c414",
   "metadata": {},
   "source": [
    "**Hypothesis:**\n",
    "\n",
    "The backchannel class b (e.g., “mm-hm”, “uh-huh”, “yeah”) will have lower average intensity (int_mean) and shorter duration (end_time − start_time) than other dialogue acts.\n",
    "\n",
    "Backchannels are brief listener signals produced with reduced effort, often low-amplitude and prosodically flat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6d84c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "Feature: int_mean\n",
      "Mean (b):     56.8067\n",
      "Mean (others):51.4039\n",
      "Mann–Whitney U (b < others): U=23718576.0, p=1.000e+00\n",
      "===\n",
      "Feature: duration\n",
      "Mean (b):     0.5717\n",
      "Mean (others):3.4102\n",
      "Mann–Whitney U (b < others): U=4411406.0, p=0.000e+00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load your speech features (valid split recommended)\n",
    "# --------------------------------------------------\n",
    "df = speech_valid.copy()   # or pd.read_csv(\"speech_features_valid.csv\")\n",
    "\n",
    "# Create duration column\n",
    "df[\"duration\"] = df[\"end_time\"] - df[\"start_time\"]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Separate backchannels (b) vs others\n",
    "# --------------------------------------------------\n",
    "b_df = df[df[\"da_tag\"] == \"b\"]\n",
    "others_df = df[df[\"da_tag\"] != \"b\"]\n",
    "\n",
    "# Pick features to test\n",
    "features = [\"int_mean\", \"duration\"]\n",
    "for f in features:\n",
    "    print(\"===\")\n",
    "    print(f\"Feature: {f}\")\n",
    "    print(f\"Mean (b):     {b_df[f].mean():.4f}\")\n",
    "    print(f\"Mean (others):{others_df[f].mean():.4f}\")\n",
    "\n",
    "    # Mann-Whitney U: one-sided (b < others)\n",
    "    u_stat, p_val = stats.mannwhitneyu(b_df[f], others_df[f], alternative=\"less\")\n",
    "    print(f\"Mann–Whitney U (b < others): U={u_stat:.1f}, p={p_val:.3e}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Visualization\n",
    "# --------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Average intensity by DA\n",
    "sns.barplot(data=df, x=\"da_tag\", y=\"int_mean\", ax=axes[0])\n",
    "axes[0].set_title(\"Average int_mean by Dialogue Act\")\n",
    "\n",
    "# Average duration by DA\n",
    "sns.barplot(data=df, x=\"da_tag\", y=\"duration\", ax=axes[1])\n",
    "axes[1].set_title(\"Average duration by Dialogue Act\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30452b48",
   "metadata": {},
   "source": [
    "From the left plot (Average int_mean by Dialogue Act) we can see that: Backchannels (b) have an average intensity of around 56–57 dB. Most other dialogue acts (sd, aa, sv, ba, fc, qy, ny) range from 58–65 dB. Especially, fc is the loudest (≈ 65 dB). (sd, aa) consistently have higher intensity than b. Only x is lower than backchannels.\n",
    "\n",
    "**Explanation: **\n",
    "\n",
    "Backchannels are meaningfully softer than typical dialogue acts. This strongly supports the hypothesis that people produce b acts quietly, reflecting their role as minimal, supportive listener feedback.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca8d13",
   "metadata": {},
   "source": [
    "From the right plot (Average duration by Dialogue Act): Backchannels (b) have a duration around 0.6 seconds, one of the shortest durations in the corpus.\n",
    "For Other categories:\n",
    "*\tsv (statements-verifiable) ≈ 3.8 s\n",
    "*\tsd (statements-non-opinion) ≈ 3.2 s\n",
    "*\tqy (questions) ≈ 2.4 s\n",
    "*\tfc ≈ 1.1 s\n",
    "*\tba ≈ 1.1 s\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Backchannels are much shorter than almost all dialogue acts. This matches real conversational behavior: backchannels are brief acknowledgments inserted between longer speaker turns. The only shorter one is ny, which also makes sense—short single-word responses like “yeah”. But even ny is a different functional category and still roughly comparable to b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b301752",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "Both predictions in your hypothesis are partially supported by the data:\n",
    "*\tBackchannels are quieter than most dialogue acts.\n",
    "*\tBackchannels are significantly shorter than other dialogue acts.\n",
    "\n",
    "This reflects their discourse function: they are low-effort, low-prominence listener signals that acknowledge understanding without taking the floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7b90e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
